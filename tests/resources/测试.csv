question,similarQuestions,answer
金融支付质量规范都包含哪些规范,,包含测试质量规范集，研发质量规范集，通用质量规范集
测试质量规范集都包含哪些规范,,包含专项测试集，业务测试集，缺陷管理规范集
研发质量规范集都包含哪些规范,,包含checklist规范，showcase规范，codereview规范，代码分支规范，自测规范，联调规范
通用质量规范集都包含哪些规范,,包含发布规范，应急规范，多栈规范，资金安全流程，安全意识规范集
业务测试规范都包含哪些,,包含众测，用例设计，
众测的流程是什么,"[""众测的必要性"", ""众测的要求"", ""众测的流程""]","1.众测必要性
通过多人测试，能够避免测试的盲点，发现更多的质量问题。使用户第一时间体验到产品功能，对产品提出改进意见，及各种bug反馈，便于产品上线后良好的用户体验及反馈。

2.众测要求
2.1 客户端APP
版本封板前两天组织众测，新功能+老功能全回归；

2.2 大中型项目
项目上线前两天组织众测，新功能+老功能全回归；

3.众测流程
3.1.众测计划邮件
包括不限于：

众测的内容：版本项目及负责人，例如掌链地图找房重构，邀请大家进行众测，体验相关功能。前提是需要跟PM沟通确认需要众测的相关功能点；
众测的范围：新项目+核心功能点；
众测包：给出iOS和Android的二维码，前提是需要亲自安装该安装包，确定是可安装并能使用的包，避免出现在众测开始无法安装下载包的问题影响下一步流程；
众测账号：需提前准备需要众测功能的相应账号，确定账号可用，避免出现在众测重无法登录问题影响下一步流程；
众测说明：时间+地点+人员；
众测环境：使用的众测环境；
已知问题：列出已知问题，避免重复提出相同问题；
列出常见问题：
             （1）例如掌链iOS设备需要开发机列表中，才能安装adhoc的包，否则只能安装homesoso的包，两者的区别是前者不可测push，后者可测push；

             （2）例如新房link在iOS设备上无法安装时，可在设置-通用-描述文件，信任即可；

其他：如与当前众测APP需要其他端有交互测试，例如新房link需要跟案场及link后台有交互测试，那么应该准备相应的案场包，以及需要绑定的host；例如新房link小流量测试需要加白名单等  ；    
3.2.众测过程
众测组织者：QA
众测参与者：相关RD、QA、PM、运营以及需要体验该功能的同事；
众测设备：需要准备相应的设备，iOS和Android互相体验对方的设备，以便发现需求实现方式不一致的问题；
众测case：需要给出xmind或者excel形式的众测功能点，以便在众测的过程中主导众测流程，同时需要注意的是不用所有众测者逐条过case，如只是一个case的几种情况，可安排众测者分别体验这几种情况即可；
众测过程中，收集相关问题，如遇紧急问题影响下一步，需要RD现场协助解决，如其他问题，则可记录；
众测过程中，后端日志QA或RD需要密切关注或进行事后check，避免问题遗漏；
众测结束后，QA总结所有问题。与PM、RD三方逐条确认，确认哪些问题遗留哪些需要修复，并邮件周知大家；
4.众测结果
众测结束后，QA负责发送一份众测结果邮件，邮件包括：

众测说明：时间+地点+参与人；
众测发现的问题：发现bug+解决状态（已解决未解决）；
后续安排：例如未解决bug一天解决完成。"
cookie测试怎么测试，有哪些内容,"[""为什么要做coor测试"", ""coor的测试内容有哪些"", ""coor和session的区别"", ""coor的测试工具有哪些""]","1.cookie简介
cookie包含我们访问的每个网站收集的有关我们的有限数据，将这些数据存储在会话期间的设备硬盘驱动器内的文本文件中，当浏览器从服务器请求信息时，访问该数据；
cookie最常用于存储访问者的用户ID或某些网站的代码，这使得用户在下次访问时访问更快捷，更方便；
cookie经常被购物网站使用，尤其是那些使用购物车的网站。营销网站利用它们根据用户以前在cookie中跟踪和记录的兴趣来个性化广告。它们也被其他有兴趣跟踪用户访问的网站使用；
cookies主要包含：发送cookie信息的服务器的名称、cookie的到期日、cookie的唯一标识符简称为cookie ID；
cookie都存储小块数据，主要分为三类; 会话cookie，持久性cookie和第三方cookie；
            a、会话cookie一旦关闭浏览器就会自动从用户的计算机中删除；

            b、持久性cookie将附加到期日期，因此将保留在用户的计算机中，直到到期或用户主动删除其cookie缓存；

            c、第三方cookie是由第三方出于研究原因安装，而不是通过用户活动实现；

cookie可以在网站运行的顺畅程度中发挥重要作用，因此通过所访问的网站测试它们如何编写并存储在硬盘中非常重要；
cookie本地存储了重要信息，所以cookie的安全性问题很重要。
2.cookie测试内容
测试点	测试内容	
1.禁止使用Cookie	设置浏览器禁止使用Cookie，访问网页后，检查存放Cookie文件中未生成相关文件；	
2.Cookie存储路径	
按照操作系统和浏览器对Cookie存放路径的设置，检查存放路径是否与设置一致；
3.Cookie过期检查	按照Cookie过期时间，检查存放文件该Cookie是否被自动删除；	
4.检查浏览器中Cookie选项	通过不同浏览器，设置是否接受Cookie文件，如同意接受Cookie，检查存放路径中是否存在Cookie文件；	
5.浏览器删除Cookie	通过浏览器的设置，删除Cookie文件；	
6.Cookie加密	提交敏感信息时，数据应加密；	
7.Cookie保存信息	验证Cookie能正常工作；	
8.篡改Cookie	修改Cookie内容，查看系统功能是否出现异常，或数据错乱；	
9.Cookie的兼容性	使用不同类型，或同一类型不同版本的浏览器，检查cookie文件的兼容性；	
10.刷新操作对cookie的影响	进行刷新操作后，是否重新生成cookie文件或是对cookie文件进行修改；	
11.检查cookie内容存储是否完整正确	
若cookie进行了加密，先对cookie文件内容进行解密，然后检查是否按照设计要求存储了相关所有的cookie记录信息；


12.对应硬盘存储空间没有空闲时，是否能进行cookie内容的有效存储；	
13.多次做相同的操作或设置，检查是否更新或添加了新的cookie文件

按照设计要求进行判断；	
14.如果使用cookie来统计次数，则要检测是否统计正确	例如通过用户登录次数进行统计；	
3.cookie使用问题
根据我们到目前为止所讨论的内容，我们可以看到，如果您在网站上使用cookie，可能会出现以下问题：

可用性	如果用户具有针对cookie的个人偏好或不确定浏览器上的cookie策略，则使用被认为太多cookie的内容可能会阻止他们访问您的网站。
安全性	在cookie中存储用户的敏感信息可能会危及或危及安全性。
功能	如果用户选择了严格的cookie策略或不确定浏览器上的cookie策略，则可能会在过度使用cookie的站点中发生故障。
4.常用cookie测试工具
   高级Cookie管理器
             这是Firefox的cookie编辑器，可作为Firefox附加组件使用；
       2.   EditThisCookie 
             广泛使用的cookie编辑器，可谷歌浏览器配合使用，并可作为Chrome扩展程序安装；
5.cookie与session的区别
数据存储	数据存放在客户的浏览器上；	数据放在服务器上；
安全	不是很安全，别人可以分析存放在本地的cookie并进行cookie欺骗；	更安全；
性能	减轻服务器性能；	会在一定时间内保存在服务器上。当访问增多，会比较占用服务器的性能；
大小限制	cookie保存有大小限制，大小不能超过4K，最多保存20个cookie；	无限制；
建议	其他信息如果需要保留，可以放在cookie中；	将登陆信息等重要信息存放为session；"
code diff 测试怎么测试，有哪些内容,"[""code diff的要求有哪些"", ""code diff有哪些关注点""]","1.目的
规范codediff测试，明确codediff测试时间节点、关注点及各角色职责范围等

2.要求
2.1 codediff项目要求：所有项目都要求进行codediff，且需要在提测后和上线前分别进行，提测后的codediff需要保证RD（FE）、QA务必参与，组织形式不限，上线前的codediff可以qa自行判断是否需要RD（FE）参与，如果有配置文件类的新增和修改，QA需要做一次double check；如果合并代码时存在解决冲突的情况，则需关注是否有漏合代码以及误删代码的情况。

2.2 codediff介入时间点：提测后，上线前

2.3 codediff参与人员：QA、RD（FE），由QA组织，和RD一起比对此次代码修改内容

新建项目：需要关注需求实现是否完善，上线前需要关注配置文件各项配置的正确性
大中项目：需要关注对历史代码的改动影响范围，以及新增配置文件的配置
小项目：需要关注代码改动点，评估代码影响范
3.内容关注点
3.1.提测后的内容关注点
业务是否如实在代码中实现	
是否符合设计
产品需求点是否实现
逻辑是否都是完整闭合的
日志的添加是否完整	
业务日志：线上日志需考虑性能相关问题（数据量、磁盘IO等）
异常日志：需要在diff过程明确catch住可能存在异常的部分
规范：日志输出符合易读的原则，不使用system.out输出
分级：日志分级别，调试使用debug，运行时日志使用info，系统遇到问题使用warn，系统遇到异常使用error
null异常，日志的输出要判断变量是否可能为空
业务监控的添加是否完整	
系统监控：常用的系统监控：cpu，load，内存，流量
业务监控：通常在关键业务节点上添加，比如常见的接口调用次数，响应市场，任务成功/失败监控以及一些异常的监控等
确保引用正确包，监控名称和后台配置一致
代码层面	
代码的基本规范是否满足，命名、日志规范
sql改了，有哪些功能受影响
某个类中的方法改了，需要确认影响的课验证的UI或系统功能
文案类的确认，和最新PRD对比
for，while循环，要检查好退出条件，避免死循环
DB设计	
字段、索引的设计，大表的查询，是否走了索引
新旧数据的处理、兼容
数据库的表结构能承受多大的数据库
合并一些相同的sql语句，提高执行效率从而减少锁表次数
对系统结构的测试	
缓存有效性：对缓存服务器清除掉的处理
外部系统调用常见处理：超时、异常、retry次数
被调用的外部接口是否可用
中间数据的测试	
提出可测性需求，添加日志输出、接口输出等
3.2.上线前的内容关注点
配置相关测试	
确认第三方服务，包括dubbo、kafka、redis等的配置、地址超时时间等
线上环境是否修改，是否正确，包括被调用方的地址，数据库地址
测试环境的配置是否使用的线上地址，反之亦然
pom文件，版本号是否正确，线上不能有snapshot版本，配置的java版本不能高于线上机器的运行版本
测试环境和线上的区别	
账户是否在线上可以使用
接口是否可以使用
db地址是否为db域名
前端接口外网应用禁止使用ip
代码层面	
代码合并是否存在错误
是否存在误删代码
4.结果记录
需要将codediff结果记录在codediff结果平台：http://hetu..com/project/tp/codediff/list"
兼容性测试怎么测试，有哪些内容,"[""兼容测试的定义是什么"", ""兼容测试的目的是什么"", ""兼容测试的内容有哪些"", ""兼容测试的要求是什么"", ""原则是什么""]","1.简介
1.1.兼容性测试定义
兼容性测试是指检查软件在一个特定的硬件、软件、操作系统、网络等环境下是否能够正常地运行，检查软件之间是否能够正确地交互和共享信息，以及检查软件版本之间的兼容性问题。包括硬件之间、软件之间和软硬件之间的兼容性。
兼容性测试是指发现软件在某个环境下不能正常使用。
兼容性测试包括两个方面的含义，第一是指待发布的软件在特定的软、硬件平台上是否能正常运行；第二是指待发布的软件对指定平台上的其他软件是否有影响，是否影响其他软件的使用(对于嵌入式的软件则不存在这个问题)。
1.2.兼容性测试目的
兼容第三方软件，确保第三方软件能正常运行，用户不受影响。

2.原则
任何发现的兼容性问题都要记录，不要因其流量小、机型少等原因不记录问题。

3.要求
用户端（C端）必须要求进行兼容性测试；
作业端（B端）选择非强制进行兼容性测试；
浏览器或者手机平台根据用户流量分析结果进行选择，比如访问TOP10的ios手机，安卓手机，浏览器等。把控侧重点，避免资源浪费。
4.测试内容
浏览器兼容	主流的浏览器，TOP10浏览器；
屏幕尺寸和分辨率兼容	合同尺寸电脑，界面，分辨率；
操作系统兼容	Mac/Windows/ios9，ios11，ios12，ios13，ios14， 安卓5，8，9，10，11等；
不同设备型号兼容	top10的机型，如安卓（华为，小米，oppo等），苹果（iphone 11，iphone 10等）；
客户端新老版本兼容	贝壳2.5.0版本，链家2.4.0版本；
数据兼容	操作系统升级/覆盖安装，客户端升级/覆盖安装；
网络兼容	wifi/ 3g/ 4g/ 慢网络/ 热点（可以通过代理工具模拟）；
不同客户端的兼容	ios/安卓/小程序/pc端；
5.测试方法
1）全手工测试兼容性；

2）借助第三方平台，例如百度众测平台、云测平台等。"
功能测试怎么测试，有哪些内容,"[""功能测试的定义是什么"", ""测试内容有哪些""]","1简介
1.1定义
1.2目的
2测试内容
2.1 APP功能测试
2.1.1运行
2.1.2应用的前后台切换
2.1.3免登录     
2.1.4 数据更新
2.1.5 离线浏览
2.1.6 App更新
2.1.7 定位、照相机服务
2.1.8 时间测试
2.1.9 PUSH测试
2.1.10 性能测试评估
2.1.11 交叉事件测试
2.1.12 用户体验测试




1.简介
1.1.定义
Functional testing（功能测试），也称为behavioral testing（行为测试），根据产品特性、操作描述和用户方案，测试一个产品的特性和可操作行为以确定它们满足设计需求。本地化软件的功能测试，用于验证应用程序或网站对目标用户能正确工作。使用适当的平台、浏览器和测试脚本，以保证目标用户的体验将足够好，就像应用程序是专门为该市场开发的一样。功能测试是为了确保程序以期望的方式运行而按功能要求对软件进行的测试，通过对一个系统的所有的特性和功能都进行测试确保符合需求和规范。
功能测试也叫黑盒测试或数据驱动测试，只需考虑需要测试的各个功能，不需要考虑整个软件的内部结构及代码.一般从软件产品的界面、架构出发，按照需求编写出来的测试用例，输入数据在预期结果和实际结果之间进行评测，进而提出更加使产品达到用户使用的要求。

1.2.目的
加强功能测试设计覆盖度；
提供多样测试手段；
2.测试内容
2.1.APP功能测试
     根据软件说明或用户需求验证App的各个功能实现，采用如下方法实现并评估功能测试过程：

采用时间、地点、对象、行为和背景五元素或业务分析等方法分析、提炼App的用户使用场景，对比说明或需求，整理出内在、外在及非功能直接相关的需求，构建测试点，并明确测试标准，若用户需求中无明确标准遵循，则需要参考行业或相关国际标准或准则。
根据被测功能点的特性列丼出相应类型的测试用例对其进行覆盖，如；涉及输入的地方需要考虑等价、边界、负面、异常或非法、场景回滚、关联测试等测试类型对其进行覆盖。
在测试实现的各个阶段跟踪测试实现与需求输入的覆盖情况，及时修正业务或需求理解错误。
2.1.1.运行
App安装完成后的试运行，可正常打开软件；
App打开测试，是否有加载状态进度提示；
App 打开速度测试，速度是否可观；
App 页面间的切换是否流畅，逻辑是否正确；
注册
 同表单编辑页面；
 用户名密码长度；
 注册后的提示页面；
 前台注册页面和后台的管理页面数据是否一致；
 注册后，在后台管理中页面提示；
登录
使用合法的用户登录系统；
系统是否允许多次非法的登陆，是否有次数限制；
使用已经登陆的账号登陆系统是否正确处理；
使用禁用的账号登陆系统是否正确处理；
用户名、口令（密码）错误或漏填时能否登陆；
删除或修改后的用户，原用户登陆；
不输入用户口令和用户、重复点（确定或取消按钮）是否允许登陆；
登陆后，页面中登陆信息；
页面中有注销按钮；
登陆超时的处理；
注销
注销原模块，新的模块系统能否正确处理；
终止注销能否返回原模块，原用户；
注销原用户，新用户系统能否正确处理；
使用错误的账号、口令、无权限的被禁用的账号进行注销；
2.1.2.应用的前后台切换
APP切换到后台，再回到app，检查是否停留在上一次操作界面；
APP切换到后台，再回到app，检查功能及应用状态是否正常，IOS4和IOS5的版本的处理机制有的不一样；
app切换到后台，再回到前台时，注意程序是否崩溃，功能状态是否正常，尤其是对于从后台切换回前台数据有自动更新的时候；
手机锁屏解屏后进入app注意是否会崩溃，功能状态是否正常，尤其是对于从后台切换回前台数据有自动更新的时候；
当App使用过程中有电话进来中断后再切换到app，功能状态是否正常；
当杀掉app进程后，再开启app，app能否正常启动；
出现必须处理的提示框后，切换到后台，再切换回来，检查提示框是否还存在，有时候会出现应用自动跳过提示框的缺陷；
对于有数据交换的页面，每个页面都必需要进行前后台切换、锁屏的测试，这种页面最容易出现崩溃。
2.1.3.免登录     
      很多应用提供免登录功能，当应用开启时自动以上一次登录的用户身份来使用app。

app有免登录功能时，需要考虑IOS版本差异；
考虑无网络情况时能否正常进入免登录状态；
切换用户登录后，要校验用户登录信息及数据内容是否相应更新，确保原用户退出；
根据MTOP的现有规则，一个帐户只允许登录一台机器。所以，需要检查一个帐户登录多台手机的情况。原手机里的用户需要被踢出，给出友好提示；
app切换到后台，再切回前台的校验；
切换到后台，再切换回前台的测试；
密码更换后，检查有数据交换时是否进行了有效身份的校验；
支持自动登录的应用在进行数据交换时，检查系统是否能自动登录成功并且数据操作无误；
检查用户主动退出登录后，下次启动app，应停留在登录界面；
2.1.4.数据更新
      根据应用的业务规则，以及数据更新量的情况，来确定最优的数据更新方案。

需要确定哪些地方需要提供手动刷新，哪些地方需要自动刷新，哪些地方需要手动自动刷新。
 确定哪些地方从后台切换回前台时需要进行数据更新。
根据业务、速度及流量的合理分配，确定哪些内容需要实时更新，哪些需要定时更新。
确定数据展示部分的处理逻辑，是每次从服务端请求，还是有缓存到本地，这样才能有针对性的进行相应测试。
检查有数据交换的地方，均有相应的异常处理。
2.1.5.离线浏览
     很多应用会支持离线浏览，即在本地客户端会缓存一部分数据供用户查看。

在无网络情况可以浏览本地数据；
退出app再开启app时能正常浏览；
切换到后台再切回前台可以正常浏览；
锁屏后再解屏回到应用前台可以正常浏览；
在对服务端的数据有更新时会给予离线的相应提示；
2.1.6.App更新
当客户端有新版本时，有更新提示。
当版本为非强制升级版时，用户可以取消更新，老版本能正常使用。用户在下次启动app时，仍能出现更新提示。
当版本为强制升级版时，当给出强制更新后用户没有做更新时，退出客户端。下次启动app时，仍出现强制升级提示。
当客户端有新版本时，在本地不删除客户端的情况下，直接更新检查是否能正常更新。
当客户端有新版本时，在本地不删除客户端的情况下，检查更新后的客户端功能是否是新版本。
当客户端有新版本时，在本地不删除客户端的情况下，检查资源同名文件如图片是否能正常更新成最新版本。如果以上无法更新成功的，也都属于缺陷。
2.1.7.定位、照相机服务
App有用到相机，定位服务时，需要注意系统版本差异。
有用到定位服务、照相机服务的地方，需要进行前后台的切换测试，检查应用是否正常。
 当定位服务没有开启时，使用定位服务，会友好性弹出是否允许设置定位提示。当确定允许开启定位时，能自动跳转到定位设置中开启定位服务。
测试定位、照相机服务时，需要采用真机进行测试。
2.1.8.时间测试
客户端可以自行设置手机的时区、时间，因此需要校验该设置对app的影响。
--中国为东8区，所以当手机设置的时间非东8区时，查看需要显示时间的地方，时间是否展示正确，应用功能是否正常。时间一般需要根据服务器时间再转换成客户端对应的时区来展示，这样的用户体验比较好。比如发表一篇微博在服务端记录的是10：00，此时，华盛顿时间为22：00，客户端去浏览时，如果设置的是华盛顿时间,则显示的发表时间即为22:00,当时间设回东8区时间时，再查看则显示为10：00。

2.1.9.PUSH测试
 检查push消息是否按照指定的业务规则发送；
检查不接受推送消息时，检查用户不会再接收到push；
如果用户设置了免打扰的时间段，检查在免打扰时间段内，用户接收不到PUSH。在非免打扰时间段，用户能正常收到push；
 当push消息是针对登录用户的时候，需要检查收到的push与用户身份是否相符，没有错误地将其它人的消息推送过来。一般情况下，只对手机上最后一个登录用户进行消息推送；
测试push时，需要采用真机进行测试。
2.1.10.性能测试评估
      App的时间和空间特性：

 极限测试：在各种边界压力情况下，如电池、存储、网速等，验证App是否能正确响应。
--内存满时安装App；
--运行App时手机断电；
--运行App时断掉网络；
响应能力测试：测试App中的各类操作是否满足用户响应时间要求；
--App 安装、卸载的响应时间；
--App 各类功能性操作的影响时间；
压力测试：反复/长期操作下、系统资源是否占用异常；
--App反复进行安装卸载，查看系统资源是否正常；
--其他功能反复进行操作，查看系统资源是否正常；
性能评估：评估典型用户应用场景下，系统资源的使用情况；
Benchmark 测试（基线测试）：与竞争产品的Benchmarking, 产品演变对比测试等。
2.1.11.交叉事件测试
针对智能终端应用的服务等级划分方式及实时特性所提出的测试方法。交叉测试又叫事件或冲突测试，是指一个功能正在执行过程中，同时另外一个事件或操作对该过程进行干扰的测试。如；
App在前/后台运行状态时与来电、文件下载、音乐收听等关键运用的交互情况测试等。交叉事件测试非常重要，能发现很多应用中潜在的性能问题。

多个App同时运行是否影响正常功能；
App运行时前/后台切换是否影响正常功能；
App运行时拨打/接听电话；
App运行时发送/接收信息；
App运行时发送/收取邮件；
App运行时切换网络（2G、3G、Wi-Fi）；
App运行时浏览网络；
App运行时使用蓝牙传送/接收数据；
App运行时使用相机、计算器等手机自带设备；
2.1.12.用户体验测试
以主观的普通消费者的角度去感知产品或服务的舒适、有用、易用、友好亲切程度。通过不同个体、独立空间和非经验的统计复用方式去有效评价产品的体验特性提出修改意见提升产品的潜在客户满意度。

是否有空数据界面设计，引导用户去执行操作；
是否滥用用户引导；
是否有不可点击的效果，如：你的按钮此时处于不可用状态，那么一定要灰掉，或者拿掉按钮，否则会给用户误导；
菜单层次是否太深；
交互流程分支是否太多；
相关的选项是否离得很远；
 一次是否载入太多的数据；
界面中按钮可点击范围是否适中；
标签页是否跟内容没有从属关系，当切换标签的时候，内容跟着切换；
操作应该有主次从属关系；
是否定义Back的逻辑。涉及软硬件交互时，Back键应具体定义；
是否有横屏模式的设计，应用一般需要支持横屏模式，即自适应设计。"
安全测试怎么测试，有哪些内容,"[""安全测试的定义是什么"", ""测试的内容有哪些"", ""测试方法有哪些"", ""测试工具有哪些""]","1.简介
1.1.定义
1.2.目的
2.测试内容
2.1.APP功能测试
2.2.专业安全漏洞检查
3.安全测试方法
3.1.安全风控业务层分析
3.2.代码层面分析
3.3.白盒
3.4抓包工具
3.5.性能测试
3.6.漏洞扫描
3.7.登录越权常规验证
3.8.SQL注入，XSS漏洞等
4.常用安全测试工具
5.贝壳安全评估申请流程
1.简介
1.1.定义
 功能测试是为了确保程序以期望的方式运行而按功能要求对软件进行的测试,通过对一个系统的所有的特性和功能都进行测试确保符合需求和规范。 功能测试也叫黑盒测试或数据驱动测试,只需考虑需要测试的各个功能,不需要考虑整个软件的内部结构及代码.一般从软件产品的界面、架构出发,按照需求编写出来的测试用例,输入数据在预期结果和实际结果之间进行评测,进而提出更加使产品达到用户使用的要求。

1.2.目的
提升功能测试方法；
丰富功能测试手段；
2.测试内容
2.1.APP功能测试
 安全测试比较专业，建议业务方通过以下相关的自查点（比较简单），来进行对安全逻辑漏洞的自查测试：

安全自查列表

测试类别

测试项

登录入口

暴力破解

验证码安全

模糊提示

弱口令

找回密码

遍历用户名

篡改手机号获取验证码

暴力破解验证码

篡改响应包

订单支付

支付过程中篡改金额

篡改商品数量

请求重放

越权

修改各类id，越权查看

权限设置是否合理

敏感信息处是否存在越权（薪资、个人信息等）

非登录状态可访问内部页面（或下载文件）

2.2.专业安全漏洞检查
面向公网的web系统及接口很可能会面临黑客的攻击，所以web系统的安全测试应尽早进行，以避免在上线后发生安全问题，建议通过以下相关的检查点，来进行对web系统的安全测试：

分类

编号

检查点

信息收集

1

检测是否能通过搜索引擎进行信息收集

2

检测是否开放有高危端口

认证授权

3

检测是否接入公司统一登录服务

4

检测是否存在暴力破解问题

5

检测注销登陆后ton是否失效

6

检测关键及敏感操作是否有二次鉴权，机制是否完善

7

检测系统是否有访问权限校验

8

检测系统中是否存在的横向越权操作

9

检测系统中是否存在的纵向越权操作

命令执行

10

检测是否存在代码/命令执行漏洞

注入攻击

11

检测是否有SQL注入漏洞。

客户端攻击

12

检测是否有XSS漏洞

13

检测是否有CSRF漏洞

文件操作

14

检测是否存在任意文件上传漏洞

15

检测是否存在任意下载，遍历系统或者web内的文件

信息泄露

16

信息泄露漏洞检查（例如：phpinfo.php、info.php、.svn、管理后台泄漏、内网信息泄漏、错误详情信息泄漏等）

17

敏感信息检查（例如：姓名、出生日期、身份证件号码、个人生物识别信息、住址、电话号码、物业地址等信息是否能批量获取，敏感数据是否脱敏等）

逻辑漏洞

18

检测是否存在各个业务上的逻辑漏洞

应用配置

19

检测是否存在弱口令

20

检测传输过程中是否采用了加密传输的机制

21

检测是否存在使用get提交敏感信息

访问控制

22

检测是否有URL跳转漏洞

 

检测是否存在SSRF漏洞

其他

23

检测是否有其他列表外的安全问题

3.安全测试方法
3.1.安全风控业务层分析
安全产品对应的风险场景 比如反爬应对的是爬虫爬去房源、楼盘的风险，用户数据的风险等；
app风控 对应的是私单飞单业务风险 从而采集客户端信息进行采集分析；
风控业务 对应如经纪人虚假带看，影子经纪人等；
3.2.代码层面分析
代码层面的漏洞分析，比如SQL注入，XSS注入等web常见安全问题；
代码性能方面，比如公司级反爬黑名单服务器稳定性等；
公司toC业务线攻击拦截等；
3.3.白盒
通过review开发代码，对代码实现功能的合理性，异常场景捕获处理方式做代码层面把控；
上线时check分支代码的改动点，是否全部覆盖，合并后影响点是否回归，合并代码是否出问题等做人工的管控；
3.4抓包工具
安全测试许多功能实现偏底层，七层和四层网关等，可借助wireshark、burpsuite等工具分析；
3.5.性能测试
性能测试介入，通过极限值，稳定性全方位保证服务的高可用；
3.6.漏洞扫描
借助工具实现全栈自动扫描，工具如AppScan
3.7.登录越权常规验证
测试主要还是通过白盒，查看代码实现方式，以及不同权限通过改动请求参数是否可模拟越权；
3.8.SQL注入，XSS漏洞等
查看代码中是否有字符串拼接实现的增删改等，通过sql请求是否出现sql注入，借助工具sqlmap检测；
XSS场景web安全漏洞覆盖，通过工具检测XSSer，BruteXSS，ezXSS；
4.常用安全测试工具
市场提供很多安全测试工具，工具只是在一定程度上帮你快速搜索到一些明显的漏洞，但漏洞的具体证实仍需手工进行检验。故不要盲目的依赖工具。具体有：

web安全	 AppScan      Burp Suite      ComeAssistant      DirBuster      DWRDigger      Fidder      HttpPutScaner      Mantra      WebScarab      WSDigger   WebInspect
HTTP/HTTPS检测	AWVS
反编译	Cavaj Java Decompiler
代码安全	Coverity      Fortify
代码检索	Search and Replace
加解密	Encoder
内存查看	Winhex
协议健壮性	Application Flood      Codenomicon      Protos      SmartBit      SIP-P      TFN      xDefend
协议抓包	Wireshark
暴力破解	John the Ripper      WebBruteForce 
Web漏洞扫描	AppScan
系统漏洞扫描	Nessus
ORACLE漏洞扫描	Checkpwd      MainAst      Nessus
DB2漏洞扫描	NGSSQuirrel
数据库安全评估工具	AppSentry
端口扫描	
Nmap

软件完整性校验	Md5sum
5.贝壳安全评估申请流程
贝壳有专门的安全团队，如果有相关安全评估需求，可以向安全团队提出申请，申请流程为：

步骤一：打开jira

打开电脑浏览器，访问JIRA，登录自己的账号。


步骤二：创建jira

点击最上方的""创建""，然后在项目中选择""安全和风控管理部""，问题类型选择""信息安全--安全测试申请""


步骤三：填写jira信息

填写对应的内容，其中带*的为必填项，填写说明见后。
填写完成后，点击右下角""创建""即可。


安全评估申请部分填写说明如下：

主题：必填项，填写申请评估的系统/项目名称，如：XXX系统申请安全评估、XXX项目申请安全评估；

模块：必选项，在下拉列表中选择申请评估的是APP或WEB；

经办人：选择提前沟通好的安全评估人员，如果没有选择自动；

项目描述：必填项，填写申请评估系统/项目的主要功能，进行简要的项目介绍；

线上环境域名：如申请评估的系统/项目已经上线使用，或已经具备线上环境域名，务必如实填写；

是否开放到外网：必选项，选择申请评估系统/项目是否开放至外网使用；

是否已上线：上线前评估选否，已经上线选是；

计划上线时间：上线前评估请填写计划上线的时间；

稳定的测试环境地址：理论上所有安全测试在测试环境中进行，代码需为最新或与线上一致，填写符合要求的稳定测试环境地址；

相关问题描述：必填项，提供对应后端测试数据库类型（如：MySQL、Oracle等）、端口、账号（密码请私聊）；提供不同权限的测试账户（如：管理员账号、普通用户账号等，每种权限至少两个账号）及对应权限的简单描述。"
性能测试怎么测试，有哪些内容,"[""性能测试目的是什么"", ""性能测试的类型有哪些"", ""性能测试的过程是什么样的"", ""性能测试的评估是什么样的"", ""性能测试的需求评估"", ""性能测试的指标分析"", ""性能指标详解"", ""性能指标参考"", ""压力与容量评估"", ""性能测试的工具有哪些""]","1   性能测试目的
性能测试的最终目标是为了最大限度的满足用户的需求，我们通常为了达到以下目标而进行性能测试：

(1)评估系统的能力：测试中得到的负荷和响应时间数据可以被用于验证所计划的模型的能力，并帮助作出决策；

(2)寻找系统瓶颈，进行系统调优；

(4)检测软件中的问题；

(5)验证稳定性、可靠性；

2   性能测试类型
在讲述性能测试类型前，先简单分析性能测试的压力模型。

随着单位时间流量的不断增长，被测系统的压力不断增大，服务器资源会不断被消耗，TPS值会因为这些因素而发生变化，而且符合通常情况下的规律。
说明：
a点：性能期望值
b点：高于期望，系统资源处于临界点
c点：高于期望，性能处于拐点
d点：超过负载，资源不够用，系统处于崩溃
通过如上模型图中的情况，我们大致可以将当前性能测试分成如下4类：性能测试、负载测试、压力测试、稳定性测试，具体的特性及描述，请参考下表：

性能测试类型：性能测试，负载测试，压力测试，稳定性测试


性能测试：

性能测试是指通过模拟生产运行的业务压力量和使用场景组合，测试系统的性能是否满足生产性能要求。

目的：验证系统是否有其宣称具有的能力。

特点：对系统性能已经有了解的前提，对需求有明确的目标，并在已经确定的环境下进行的。

关注的是系统性能是否和具体的性能需求相一致，而当系统性能超过性能需求的时候，系统的表现并不是测试人员关心的重点。

负载测试：

是指对系统不断地增加压力或增加一定压力下的持续时间，直到系统的某项或多项性能指标达到安全临界值，例如某种资源已经达到饱和状态等

目的: 找到系统处理能力的极限。了解系统的性能容量，或是配合性能调优来使用。

1.得出线下系统最大TPS。

2.得出线下系统最大TPS时系统资源利用率。

3.得出线下系统极限并发数。

压力测试：

压力测试是评估系统处于或超过预期负载时系统的运行情况。压力测试的关注点在于系统在峰值负载或超出最大载荷情况下的处理能力。

目的：检查系统处于大压力性能下时，应用的表现。

特点：一般通过模拟负载等方法，使得系统的资源使用达到较高的水平。

关注点:发现功能测试不能发现的非功能性缺陷。

产出：

1.得出线下系统崩溃点的TPS。

2.得出线下系统崩溃时资源使用率

3.得出线下系统极限并发数

稳定性测试：

在给系统加载一定业务压力的情况下，使系统运行一段时间，以此检测系统是否稳定。

目的：主要目的是验证是否支持长期稳定的运行。

关注系统稳定性。

得出系统稳定状态下的资源利用、连接池、TPS、响应时间、DB健康情况等数据。

3   性能测试基本过程
性能测试从实际执行层面来看，测试的过程一般分为这么几个阶段 

1.性能需求分析

性能需求分析是整个性能测试工作开展的基础，如果连性能的需求都没弄清楚，后面的性能测试工具以及执行就无从谈起了。

在这一阶段，性能测试人员需要与PM、DEV及项目相关的人员进行沟通，同时收集各种项目资料，对系统进行分析，确认测试的目标。并将其转化为可衡量的具体性能指标。

测试需求分析阶段的主要任务是分析被测系统及其性能需求，建立性能测试数据模型，分析性能需求，确定合理性能目标，并进行评审；

2.性能测试准备

主要包括：设计场景，根据场景编写程序、编写脚本、准备测试环境，构造测试数据，环境预调优等；

针对系统的特点设计出合理的测试场景。为了让测试结果更加准确，这里需要很细致的工作。如建立用户模型，只有知道真实的用户是如何对系统产生压力，才可以设计出有代表性的压力测试场景。这就涉及到很多信息，如用户群的分布、各类型用户用到的功能、用户的使用习惯、工作时间段、系统各模块压力分布等等。只有从多方面不断的积累这种数据，才会让压力场景更有意义。最后将设计场景转换成具体的用例。

测试数据的设计也是一个重点且容易出问题的地方。生成测试数据量达到未来预期数量只是最基础的一步，更需要考虑的是数据的分布是否合理，需要仔细的确认程序中使用到的各种查询条件，这些重点列的数值要尽可能的模拟真实的数据分布， 否则测试的结果可能是无效的。

预调优指根据系统的特点和团队的经验，提前对系统的各个方面做一些优化调整，避免测试执行过程中的无谓返工。比如一个高并发的系统，10000人在线，连接池和线程池的配置还用默认的，显然是会测出问题的。

3.执行性能测试

执行阶段工作主要包含两个方面的内容：一是执行测试用例模型，包括执行脚本和场景；其次测试过程监控，包括测试结果、记录性能指标和性能计数器的值

4.结果分析与性能调优

发现问题或者性能指标达不到预期，及时的分析定位，处理后重复测试过程。性能问题通常是相互关联相互影响的，表面上看到的现象很可能不是根本问题，而是另一处出现问题后引起的反应。这就要求监控收集数据时要全面，从多方面多个角度去判断定位。调优的过程其实也是一种平衡的过程，在系统的多个方面达到一个平衡即可。

5.性能报告与总结

编写性能测试报告，阐明性能测试目标、性能结果、测试环境、数据构造规则、遇到的问题和解决办法等。并对此次性能测试经验进行总结与沉淀。具体性能测试报告的编写可以参考《性能测试报告模板》。

上面所有内容中，如果排除技术上的问题，性能测试中最难做好的，就是用户模型的分析。它直接决定了压力测试场景是否能够有效的模拟真实世界压力，而正是这种对真实压力的模拟，才使性能测试有了更大的意义。可以说，性能测试做到一定程度，差距就体现在了模型建立上。


至于性能问题的分析、定位或者调优，很大程度是一种技术问题，需要多方面的专业知识。数据库、操作系统、网络、开发都是一个合格的性能测试人员需要拥有的技能，只有这样，才能从多角度全方位的去考虑分析问题。

4   性能评估模型
4.1 性能评估模型概述
我们的系统性能到底能不能够支撑线上真实大量的订单交易？

我想，这是我们每一个同学都很关心的问题，也是性能评估模型篇需要解答的最终问题。所以我们就带着这个问题来一步步深入性能测试。

本问题的难度不在于一个简单的结果，而在于答案背后的一系列性能测试的评估数据和算法，以及如何建立一个良好可持续的“性能评估模型”。

通常来讲，性能测试是指通过自动化的测试工具模拟多种正常、峰值以及异常负载条件来对系统的各项性能指标进行测试。

而要回答“能否支撑线上真实的订单交易”这样带有预测性的问题，实际上还需要用上另一种手段，即“性能预测”，而“性能评估模型”就是用来做性能预测的。

在预测之前，我们先来做一个数据分析，通过这个分析我们可以大概了解线上与线下的推算过程。

2013年11月11日，支付宝实现了当天交易总金额350亿元，订单总数1.8亿笔（其中手机支付占24%），活跃用户1.2亿。（来源：支付宝官方微博http://weibo.com/1627897870/AiiAjEwHO）

显然这是一个非常震惊的数字，它见证着电商的今天也预示着电商的未来。

假设有一天，我们的产品也同样需要面对如此巨大的成交量，当然这会是我们的骄傲。可问题也会随之而来，我们的系统能不能撑得住？我们有多少个性能瓶颈？我们应该给哪些服务加机器？我们的性能测试应该如何做？我们应该设定哪些性能指标？稳定性99%够不够？等等。希望在这一天到来之前我们就已经很好的解决了上面提到的问题。针对这个数字，下面我们就一起来剖析数字背后的性能情况。

双11当天，支付宝的订单数是1.8亿笔，意味着每小时订单数达到1.8亿 / 24 = 750万笔，也意味着每秒订单数达到750万 / 3600 = 2083笔。

首先，让我们先来看看吞吐率指标（TPS），假设支付宝有100台前置服务器，分到每台机器就是每秒处理订单2083笔 / 100 = 20笔。

这是不是意味着只要单台服务器的性能达到20 tps，我们的线上服务就足以支撑每天1.8亿的交易量呢？

答案显然不是。

我们先来看看淘宝网每天交易量分布：

淘宝网日常每天交易量分布

可以看到，交易量并不是24小时平均分布的，从早上8点开始到晚上12点才是交易量发生的主要时间段，也就是说实际上每天只有2/3的时间（16小时）才是有效时间。我们的性能指标应该调整到20tps * 150% = 30tps。
即使是16个小时中，交易量也不是平均的，在午饭和晚饭时间会出现两个波谷，晚上8点到11点之间是一天的最高峰。最高峰大约是平均值的1.2倍，如果我们不想损失掉高峰时期的交易量的话，我们的吞吐率指标需要再上调到30tps * 1.2 = 36tps。

以上只是一个普通日子的交易量分布情况，如果有某些重大活动事件，比如限时抢购、定时秒杀等活动，那么我们很可能在短时间承受几倍甚至十几倍的压力。如下图是某年某活动当天的交易量分布图：

某活动当天交易量分布图

总体交易量是平日的几倍到十几倍，交易量从零点开始就达到一个小高峰，凌晨2点后逐渐降到冰点，早上8点又开始了一天的持续热度，下午7点出现一个小波谷紧接着就迎来了晚上11点左右的最高峰。对于这种特别庞大的活动我们需要提前做好性能预案，以往的数据就是我们最好的参考。

通常来讲，我们对于系统的性能要求在每天高峰时期的1.5倍到4倍，如果按2倍计算，那么我们的吞吐率指标就继续上调到 36tps * 2 = 72tps。

好了，单台服务器的性能达标了72tps。但是100台服务器加起来就能承载 72tps * 100 = 7200tps吗？

比较难，这依赖于我们的负载均衡算法。假设我们有一个还不错的算法，但也不能保证长时间绝对的公平，所以单台服务器还需要做好压力波动的缓冲，如果按1.5倍计算，这样我们的指标又要上调到72tps*150% = 98tps。

这里，我们已经把对单台服务器的性能指标从20tps逐渐上调到近100tps，是原来的5倍。当然这里还远没有结束。这1.8亿笔只是单一的下单操作的PV，假设用户每下单一笔，需要伴随着10次查询，3次提交，那么我们的服务器需要再评估其他或查询或提交的性能。假设这些其他附带请求的压力总和可能还得增大5倍，即达到98tps * 5 = 500tps。

还没完，这只是100台前置服务器，后面还有我们的核心业务层、消息系统、数据库系统、第三方依赖等等。它们中的任何一方都有可能成为性能瓶颈，虽然我们的分析假设了对外依赖都不是瓶颈的情况，但实际上往往就是某些不起眼的有限资源的依赖导致了整体吞吐率的下降。

这是一个水桶效应，装水量是由最小的那块板决定的。显然我们有必要对每一个子系统（公司内和公司外）分别做单独的性能测试和评估。

完了吗？可能还没有。假设我们的稳定性足够好，好到99.99%，即使这样，在面对超大量订单（1.8亿）的时候也还是意味着有1.8万个订单是失败的。

每一次失败对用户来讲都是一次糟糕的体验，可能还会因此引起一些连锁反应。所以稳定性也是系统性能的重要指标，我们做性能测试不能仅仅关注数量，还要关注质量。

对了，既然我们已经提到用户体验，那就不得不谈响应时间（RT）了，这是性能指标的又一个维度。

一般用户不会关心你有多少台服务器，也不会关心服务器能支撑多大的交易量，他们只关心你让他等了多长的时间。关于这方面的研究有不少，较新的观点可能是：

谷歌发现，搜索页面一旦慢了5秒钟，就会导致流量降低20%
亚马逊已经表明，每100毫秒的延迟会导致他们有1%的销售损失
沃尔玛宣称加载时间每降低1秒会提升2％的转化率
用户的耐心似乎一直在下降。不管怎么样，从性能角度上看，用户的诉求就是一句话：“不要让我等！”。

为了不让用户等，我们的性能测试需要针对不同场景给出不同的考察指标，比如前端页面加载时间，后端系统响应时间，并统计出最大、最小、平均值、标准差等等。

好吧，到这里我们已经层层分析，从TPS开始到用户体验上。需要注意的是，在分析中我们使用了一些模糊的参数，比如前面说1.5倍到4倍，那到底应该是多少呢？

我想不同的业务、不同的架构、甚至不同的工程师，都有可能导致不同的结果。

而对于性能预测来讲，最重要的是建立起一个系统良好的可持续的“性能评估模型”，将线上和线下性能跟踪形成闭环，并不断调整动态发展，那样我们就可以适应任何的差异化，就算某一次我们评估错了，我们可以及时调整纠正，下一次我们就能更准了。

4.2 闭环流程图
一个可持续的良好模型需要测试流程上的配合，下图是一个将线下性能测试与线上性能状况相连接的闭环流程图。

与传统流程不同点在于：

性能测试的终点不再是发布上线，上线之后我们仍然会继续跟踪性能情况，并将结果用于下一次性能测试需求分析。
将线下与线上真正关联起来，并通过换算系数实现性能预测，系数可动态调整的。
全程性能跟踪，在生产环境中也加入性能监控，更快的解决线上性能问题。


4.3 响应时间
后端核心服务响应时间不超过100ms
后端一般服务响应时间不超过300ms
前端用户页面加载时间不超过3s

注：这里是指平均响应时间。但性能测试报告必须给出最大、最小、平均、方差等数值。
4.4 稳定性要求
确保零崩溃，9% 的稳定性。

零崩溃是指在任何非外部原因的情况下，不管运行多长时间，都不能出现程序崩溃的情况。


99.9%的稳定性，意味着每千条交易中只能容许一条出错，也意味着任何可能导致稳定性不达标的改进都需要慎重考虑。比如我们引入一种新的技术能使TPS增大50%，但是稳定性下降到97%，那么这可能也不能接受。

4.5 性能测试环境要求
为了更准确的预测性能，我们必须尽可能的保证性能测试环境的稳定。

性能测试环境要求真实的物理机，一般不能是虚拟机。
压力工具与被测环境不能在同一台机器部署。
环境上尽可能的避免其他服务影响。

测试环境准备是一个很重要又很繁琐的工作，它的效率提升依赖于我们的环境搭建平台的建设。

4.6 并发量与吞吐量（TPS）的关系
并发量（或叫并发用户数）：是指同一时间点对业务功能同时操作的用户数。

吞吐量（TPS）：一段时间内系统处理用户的请求数量。

虽然两者关系并不紧密，但是往往会有同学将它们搞混淆。其实我们可以用多线程模型来解释他们，并发量就相当于线程数，吞吐量就是所有线程处理请求数之和。

通常我们做性能测试主要考察和评估的是TPS，而并发量是一个相对较虚的概念，比如我们说一个系统支持同时1k人操作，一般不在评估的范围内。

5   性能测试需求分析
对于性能测试，一般来说工具的使用不是大的问题，问题是前期的需求分析，要不要做性能测试？性能测试的目的是什么？要做或不做性能测试的理由是什么？如何来做性能测试?这些问题的答案都是通过分析性能测试需求得到的。以下将阐述性能需求分析的一些基本方法。

5.1 业务调研
为了合理评估与制定系统的性能指标，业务调研是一个必备的过程，业务调研主要包含以下几个方面的内容：

系统信息调研
系统类型：系统的基本特性，如交易处理型系统、数据处理型系统等
架构部署：系统的整体架构、服务器部署方式
技术信息：系统运行平台、数据库产品、使用的中间件、协议及通讯方式等
业务信息：支持的业务类型、业务范围与功能、与其它系统的业务关系等
系统历史运行情况：目标TPS，用户数、PV等数据
系统数据规模：将来系统使用规模，历史系统数据规模
业务信息调研
基本业务功能：系统的基本业务概念以及系统的业务种类与具体功能
关键业务逻辑处理流程：关键业务的业务流程、交易路径、交易数据、交易流程与时序图
交易列表：调查业务系统全部交易清单，了解交易的组合关系、执行顺序等
交易量信息：在不同时间粒度下统计单个交易处理量以及总交易量信息
业务目标/业务拓展计划：目前的生产业务量和用户数以及系统预期业务目标和本次测试预期业务指标
文档资料调研
功能规格说明书
系统设计文档
生产运营统计
前期系统测试资料
业务调研涉及到的角色有以下几个：业务人员、开发人员、客户、产品人员、运维人员、DBA等。

通过业务调研至少可以得到以下几个方面的产出：

项目背景
系统架构说明
系统拓扑说明
测试范围说明
交易路径描述
需要测试的特性
不需要测试的特性
5.2 性能需求评估
在实施性能测试之前，需要对被测项目做相应的评估。主要目的是明确是否需要做性能测试和确立性能点，明确该测什么、期望值是多少。测试期望值也会根据情况评估，要求被测系统能满足将来一定时间段的压力。

判断是否进行性能测试可以从以下几个方面进行思考：

a、从业务角度来分析，如果一个项目上去后使用的人数比较多，量比较大，就有做性能测试的必要，反之，如果一个项目上线后，没有几个人在用，无论系统多大，设计如何复杂，并发性的性能测试是没有必要做的，前期可以否决。

b、从系统架构角度来分析，如果一个系统采用的框架是老的系统框架，只是在此框架上增加一些应用，其实是没有必要做性能测试。如果一个系统采用的是一种新的框架，可以考虑做负载测试。

c、从实时性角度来分析，如果一个项目要求某个功能的响应时间，这个有作并发测试的可能性，在大并发量的场景下，查看这个功能的响应时间。

d、从数据库角度分析，很多情况下，性能测试是大数据量的并发访问、修改数据库，而瓶颈在于连接数据库池的数量，而非数据库本身的负载、吞吐能力。这时，可以结合DBA的建议，来决定是否来做性能测试。

如果要进行性能测试，接下来我们就需要确定相应的性能点。主要从以下 4 个维度进行确定：

关键业务。
首要维度，是确定被测项目是否属于关键业务，有哪些主要的业务逻辑点，特别是跟交易相关的功能点。例如快捷签约、交易等接口。如果项目（或功能点）不属于关键业务（或关键业务点），则可转入第二、三、四个维度。

日请求量。
第二个维度，是界定被测项目各功能点的日请求量。如果日请求量很高，系统压力很大，而且又是关键业务，该项目需要做性能测试；而且其关键业务点，可以被确定为性能点。

逻辑复杂度。
第三个维度，是判定被测项目各功能点的逻辑复杂度。如果一个主要业务的日请求量不高，但是逻辑很复杂，则也需要通过性能测试。原因是，在分布式方式的调用中，当某一个环节响应较慢，就会影响到其它环节，造成雪崩效应。

运营推广计划。
第四个维度，是根据运营的推广计划来判定待测系统未来的压力。未雨绸缪、防患于未然、降低运营风险是性能测试的主要目标。被测系统的性能不仅能满足当前压力，更需要满足未来一定时间段内的压力。因此，事先了解运营推广计划，对性能点的制定有很大的作用。

例如，运营计划做活动，要求系统每天能支撑多少 PV、多少 UV，或者一个季度后，需要能支撑多大的访问量等等数据。

当新项目（或功能点）属于运营重点推广计划范畴之内，则该项目（或功能点）也需要做性能测试。

5.其它

以上 4 个评估维护，是相辅相成、环环相扣的，它们合成一个维度集。在实际工作中应该具体问题具体分析。例如，当一个功能点不满足以上 4 个维度，但又属于内存高消耗、CPU高消耗时，也可列入性能测试点行列。

5.3 性能测试指标
性能需求分析一个很重要的目标就是需要确定后期性能分析用的性能指标，性能指标有很多，可以根据具体项目选取和设定，而具体的指标值则需要根据业务特点和上述的一些方法进行设定。性能评估模型章节中也给了一些指标换算的基本方法。

5.3.1  性能指标分析
判断一个系统的性能通常会取决于这三个最重要的性能指标：

吞吐率（TPS）
响应时间（RT）
系统资源利用率（Load）


通常经验告诉我们，它们三者的趋势会如下图所示：

当处于轻负载区的时候，压力有多大，吞吐率就有多大。

一旦进入重负载区，就算是再怎么努力可能也会是白费。

如果到了崩溃区，那我们的努力就开始有反效果。


同样，对于我们做性能测试来讲，我们的目的就是找出这三个区域的分界线来。

所以在设计上，我们需要不断地递增负载来观察和评估系统性能情况。

可以看出，TPS低于400的时候基本属于轻负载区，400到800之间属于重负载区，高于800基本上属于崩溃区了。

5.3.2  性能指标详解
该章节详细介绍性能测试过程中需要关注的性能指标，以及性能指标的范围。本指南中的所有的性能指标数据均可用指南中的监控工具进行收集。详细方法见监控工具章节。

5.3.2.1业务性能指标
指标名

指标说明

采集方法

并发用户数

在同一时刻与服务器进行了交互的在线用户数量

Jmeter/loadrunner

响应时间

客户发出请求到得到响应的整个过程的时间。一般他可以分为三部分：呈现时间，数据传输时间和系统处理时间

Jmeter/loadrunner

吞吐量

一次性能测试过程中网络上传输的数据量的总和

Jmeter/loadrunner

每秒处理事务数(TPS)

每秒钟系统能够处理事务或交易的数量，它是衡量系统处理能力的重要指标。

Jmeter/loadrunner

点击率

点击率可以看做是TPS的一种特定情况。每秒钟用户向web服务器提交的HTTP请求数。

Jmeter/loadrunner

事务成功率

本次测试中出现成功的事务数量/事务的总数

Jmeter/loadrunner

5.3.2.2应用服务器及硬件性能指标
指标名

指标说明

采集方法

CPU占用率

对一个时间段内CPU使用状况的统计。

建议：<75%

nmon/vmstat/top

Load Average

一段时间内CPU正在处理以及等待CPU处理的进程数之和的统计信息，也就是CPU使用队列的长度的统计信息。

建议：<0.7*CPU个数*核数

top/uptime

Paging rate

内存页交换率，建议<80%

nmon/vmstat/top

磁盘I/O 

Iowait<30%

nmon/iostat/sar

SWAP

有没有交换页面

nmon

Tomcat启动的总线程数

关注线程数会不会无限制增长、线程数量是否足够

Top

Full GC次数

关注Full GC次数，以及Full GC后内存占用有没有明显增长

Jstat/jconsole

JVM内存使用与回收



Jstat/jconsole

JDBC监控



Jprofiler

Exception日志监控

监控压力过程中是否有异常日志产生

Tail&grep

5.3.2.3数据库性能指标
指标名

指标说明

采集方法

DB cpu占用率

数据库cpu利用率，建议<70%

nmon

DB load

数据库服务器负载

Top

DB mem

内存使用是否平稳

Nmon

DB 磁盘与I/O

I/O是数据库性能一个非常重要的因素，建议IOwait<30%

Nmon/iostat/sar

数据库线程数

Top

缓存命中率

点击率可以看做是TPS的一种特定情况。每秒钟用户向web服务器提交的HTTP请求数。

AWR

共享池命中率

假如这个值低于95%就要考虑调整应用（改写多为变量绑定）或者增加内存

AWR

Top 耗时 sql

找出性能较差的sql，进行优化

AWR

5.3.3  性能指标参考
在确定性能指标的时候，可以参考一下表中对应的数值。

指标项

优秀

中等

差

TPS

TPS >=100

50<TPS <100

TPS<=50

响应时间

RT<=200ms

200ms<RT<500ms

RT>=500ms

响应长度

ResponseSize<=100KB

100KB<ResponseSize<500KB

ResponseSize>=500KB

CPU

CPU<=75%

75%<CPU<80%

CPU>=80%

Load

Load<=2

2<Load<5

Load>=5

FullGC

FullGC<=1

1<FullGC<10

FullGC>=10

DBQPS

DBQPS<5*TPS

5*TPS<DBQPS<10*TPS

DBQPS>=10*TPS

慢SQL数量

慢SQL=0

慢SQL=1

慢SQL>=2

5.4 压力与容量预估
首先，由产品同学给出线上交易量预估，并根据TPS线上线下换算公式，得出线下TPS基线。



达标目标：TPS基线必须在轻负载区区域内。


面对数据库或第三方依赖，需要评估可能的瓶颈，数据库最好使用真实的并容量与线上最大值保持一致，因为数据库成为瓶颈的可能性比较大。



假如我们有一个数据库线上有数据100G，分了4个库，每个库128张表，平均每张表有195M数据，约15w条数据。

性能上我们可以只构造一个库的情况，约25G的数据。我们在做性能测试之前必须提早做好这25G的数据。

其他第三方依赖如果已经做过良好的性能评估，比如内部消息系统服务，那么我们可以适当采用性能桩的方式来替代真实后端。

注：不管是压力还是容量，都离不开具体的业务分析，只能这样才能把性能测试做到最完美。

6   性能测试准备
6.1 性能测试环境要求
当做完性能需求分析之后，就要申请性能测试环境。

因为OP运维的测试环境不允许进行性能测试，所以性能测试一般在物理测试机上运行。

6.2 环境清理
在部署系统之前必须要做的一件事就是环境清理，最简单的就是统统删除然后重新搭建一个干净无污染的系统。

如果是在旧系统上做更新，那至少也得把Log日志清理一下、其他可能的干扰进程该杀就杀掉、定时跑的任务、临时文件、初始化文件等等该清理的都清理。

6.3 环境搭建及数据准备
环境搭建理想的情况是使用我们的环境搭建平台，或者一键式环境搭建脚本。当然，如果都没有的话，我们就得按照我们的上线步骤流程来一步步搭建了。

特别注意当对数据库、消息系统、第三方服务等有依赖的时候，需要及时考虑是采用哪种方案。一是用搭建全套子系统的方式，二是用AK47中的MockServer的解决方案，即部署压力桩。

数据准备更是要提前做，比如为了模拟线上大量数据情况，我们需要灌入3kw订单数据到Oracle。这可能需要花几天时间。

6.4 压力工具选择
当我们做了性能需求分析、制定了测试方案，这时候需要选取一款合适的性能测试工具，并通过这个工具快速高效的完成测试任务。

通常我们用的压力工具，如：ab、JMeter、LoadRunner、ngrinder等工具，这些在网上都有各种的使用方法，这里就不再一一介绍了。

我们需要了解不同的压力工具的特点。

比如apache的ab，它是采用了Linux 2.6内核之后引入的epoll模型，能够制造非常高的压力，尤其是在高并发的环境下最能体现出它的优势。如果我们要压某个耗时稍长的请求，比如某个css静态文件，ab是非常合适的。Ab的缺点是不够灵活。

Jmeter

基于UI操作，容易上手，但是编程能力较弱（使用beanshell脚本语言）。
其次JMeter基于线程，模拟数千用户几乎不可能。
LoadRunner

基于UI操作，容易上手。早期很流行，功能强大，但是太笨重，安装很麻烦。
不开源，扩展性不高，收费贵。往后的方向肯定是客户端工具逐步向平台化发展，所以已经慢慢被替代了。

Ngrinder

单节点支持3000并发、支持分布式、可监控被测服务器、可录制脚本、开源、平台化。

Jmeter、loadrunner和ngrinder对比


6.5 资源监控工具部署
Linux系统资源、JVM等监控工具非常多，不过非常遗憾的是很难有一种工具都大面积适用。

Linux系统资源可以利用op提供的平台能力

JVM监控可以选用jstat、jmap、jstrace等工具。

对数据库也需要监控，错误的SQL、慢查询SQL等都是很重要的线索，具体监控形式还需要与DBA协商。

7   性能分析与调优
7.1 性能分析
性能结果分析是性能测试中的一个重要部分，同时也是一个难点。由于不同的软件系统，不同的性能指标，结果分析方法都是不一样的。需要具体问题具体分析。下面将阐述一些性能分析的方法与建议。

7.1.1  性能分析的目的
1）找出系统瓶颈（硬件、软件）

2）提出性能优化方案

3）达到合理的硬件和软件配置

4）使系统资源使用达到最大平衡

7.1.2  常见性能瓶颈征兆
在性能测试执行过程中，我们需要观察和了解系统的运行状态，如果出现以下征兆，则表示系统可能存在瓶颈。 

1)持续缓慢：应用程序一直特别慢，改变负载，对整体响应时间影响很少； 

2) 随着时间推进越来越慢：负载不变，随着时间推进越来越慢，可能到达某个阈值，系统被锁定或出现大量错误而崩溃； 

3)随着负载增加越来越慢：每增加若干用户，系统明显变慢，用户离开系统，系统恢复原状； 

4)零星挂起或异常错误：可能是负载或某些原因，用户看到页面无法完成并挂起，无法消除； 

5)可预见的锁定：一旦出现挂起或错误，就加速出现，直到系统完全锁定。通常要重启系统才解决。 

6)突然混乱：系统一直运行正常，可能是一个小时或三天之后，系统突然出项大量错误或锁定。

7.1.3  性能数据解读建议
性能分析过程也是一个解读数据的过程，读懂了数据你就能知道问题出在何处。随着经验的累积将会很容易判断问题的根源，甚至在开发阶段就能对可能出现问题的点打预防针。

性能指标类型

标准

性能瓶颈征兆

分析工具

TPS及其波动范围

1.Tps符合性能目标

2.Tps轨迹波动平稳

1.TPS有明显的大幅波动，不稳定。例如TPS轨迹缓慢下降，缓慢上升后骤降，呈瀑布型，呈矩形，分时间段有规律的波动，无规律的波动等。这些TPS的波动轨迹反映出被测试的性能点存在性能瓶颈，需要性能测试工程师与开发工程师查找性能瓶颈的原因。

2. TPS轨迹比较平稳，但是也存在波动现象。该类波动不明显，很难直接确定是否存在性能瓶颈。我们需要根据其他指标来进行判断。

Jmeter/loadrunner

响应时间

90%平均事务响应时间<性能目标

1.关注高峰负载时，用户操作响应时间； 

 2.关注数据库增量，对用户操作响应时间的影响。

Jmeter/loadrunner

Web\DB服务器内存

 

1.很高的换页率

2.进程进入不活动状态;

3.交换区所有磁盘的活动次数过高;

4.过高的全局系统CPU利用率;

5.内存不够出错(out of memory errors)

Nmon/top/vmstat

WEB\DB服务器CPU

合理使用的范围在60%至70%

1.很慢的响应时间

2.CPU空闲时间为零

3.过高的用户占用CPU时间

4.过高的系统占用CPU时间

5.长时间的有很长的运行进程队列

Nmon/vmstat/top

WEB\DB服务器磁盘I/O

Iowait<30%

1.过高的磁盘利用率；

2.太长的磁盘等待队列；

3.等待磁盘I/O的时间所占的百分率太高；

4.太高的物理I/O速率；

5.过低的缓存命中率；

6.太长的运行进程队列，但CPU却空闲；

Nmon/sar/iostat

Oracle数据库

 

1.缓存命中率小于0.90

2.top 10sql耗时高

Oracle数据库的分析和优化，是一门专门的技术，进一步的分析可查相关资料，也可以查看数据库性能分析工具AWR章节。

AWR

7.1.4  如何定位性能问题
性能问题的定位排查过程比较复杂，可以采用“拆分问题，隔离分析”的方法进行分析，即逐步定位、从外到内、从表及里、逐层分解、隔离排除。以下分析顺序可供参考。

 日志分析--->服务器硬件瓶颈---〉网络瓶颈（对局域网，可以不考虑）---〉服务器操作系统瓶颈（参数配置）---〉中间件瓶颈（参数配置，web服务器等）---〉数据库及应用瓶颈（SQL语句、数据库设计、业务逻辑、算法等）。

以上过程并不是每个分析中都需要的，要根据测试目的和要求来确定分析的深度。整个过程中，要配套使用一些健康工具和日志进行。如： JDK自带的Jconsole，或者JProfiler，来监控服务器性能，oracle的监控工具awr等，具体工具可以参考工具篇。

另外，做性能测试的时候，我们一定要确保瓶颈不要发生在自己的测试脚本和测试工具上。

基于上述思想的指导，在具体执行层面，可以参考如下分析过程：

首先，当我们系统有问题的时候，我们不要急于去调查我们代码，这个毫无意义。我们首要需要看的是操作系统的报告。看看操作系统的CPU利用率，看看内存使用率，看看操作系统的IO，还有网络的IO，网络链接数，等等。通过观察这些数据，我们就可以知道我们的软件的性能基本上出在哪里。比如：

　　1）先看CPU利用率，如果CPU利用率不高，但是系统的Throughput和Latency上不去了，这说明我们的程序并没有忙于计算，而是忙于别的一些事，比如IO。（另外，CPU的利用率还要看内核态的和用户态的，内核态的一上去了，整个系统的性能就下来了。而对于多核CPU来说，CPU 0是相当关键的，如果CPU 0的负载高，那么会影响其它核的性能，因为CPU各核间是需要有调度的，这靠CPU0完成）

　　2）然后，我们可以看一下IO大不大，IO和CPU一般是反着来的，CPU利用率高则IO不大，IO大则CPU就小。关于IO，我们要看三个事，一个是磁盘文件IO，一个是驱动程序的IO（如：网卡），一个是内存换页率。这三个事都会影响系统性能。

　　3）然后，查看一下网络带宽使用情况，在Linux下，你可以使用iftop，iptraf，ntop，tcpdump这些命令来查看。或是用Wireshark来查看。

　　4）如果CPU不高，IO不高，内存使用不高，网络带宽使用不高。但是系统的性能上不去。这说明你的程序有问题，比如，你的程序被阻塞了。可能是因为等那个锁，可能是因为等某个资源，或者是在切换上下文。

通过了解操作系统的性能，我们才知道性能的问题，比如：带宽不够，内存不够，TCP缓冲区不够，等等，很多时候，不需要调整程序的，只需要调整一下硬件或操作系统的配置就可以了。具体配置项的调优，可以参考配置项调优参考章节。

接下来，我们需要使用性能检测工具，也就是使用某个Profiler来差看一下我们程序的运行性能。如：Java的JProfiler/TPTP/CodePro Profiler，GNU的gprof，IBM的PurifyPlus，Intel的VTune，AMD的CodeAnalyst，还有Linux下的OProfile/perf，后面两个可以让你对你的代码优化到CPU的微指令级别，如果你关心CPU的L1/L2的缓存调优，那么你需要考虑一下使用VTune。使用这些Profiler工具，可以让你程序中各个模块函数甚至指令的很多东西，如：运行的时间，调用的次数，CPU的利用率，等等。这些东西对我们来说非常有用。

我们重点观察运行时间最多，调用次数最多的那些函数和指令。这里注意一下，对于调用次数多但是时间很短的函数，你可能只需要轻微优化一下，你的性能就上去了（比如：某函数一秒种被调用100万次，你想想如果你让这个函数提高0.01毫秒的时间，这会给你带来多大的性能）

使用Profiler有个问题我们需要注意一下，因为Profiler会让你的程序运行的性能变低，像PurifyPlus这样的工具会在你的代码中插入很多代码，会导致你的程序运行效率变低，从而没发测试出在高吞吐量下的系统的性能，对此，一般有两个方法来定位系统瓶颈：

1）在你的代码中自己做统计，使用微秒级的计时器和函数调用计算器，每隔10秒把统计log到文件中。

2）分段注释你的代码块，让一些函数空转，做Hard Code的Mock，然后再测试一下系统的Throughput和Latency是否有质的变化，如果有，那么被注释的函数就是性能瓶颈，再在这个函数体内注释代码，直到找到最耗性能的语句。

最后再说一点，对于性能测试，不同的Throughput会出现不同的测试结果，不同的测试数据也会有不同的测试结果。所以，用于性能测试的数据非常重要，性能测试中，我们需要观测试不同Throughput的结果。

7.1.5  常见性能问题参考
下面是整理收集的一些常见问题列表，不全，也许并不对，大家可以补充指正。

类别

常见性能问题

操作系统类

1. Sys的CPU使用率过高

2. User的CPU使用率过高，持续大于80%以上

3.可用物理内存不足导致内存溢出

4. 磁盘空间不足导致交易处理失败，性能下降

5. TCP/IP连接数限制导致用户请求失败

6.磁盘IO使用比较繁忙，持续大于70%

中间件类

常用主流中间件：Tomcat、apache、nginx、Weblogic、Jboss等

1.线程不回收导致溢出，引发宕机

2.数据库连接池不释放导致溢出

3.JVM内存参数设置不合理，新生代过大或偏小

4.永久代设置过小，导致栈溢出

5.其它问题

应用程序类

1.程序响应时间超长

2. JAVA程序内存溢出，内存中存放大量数据对象

3. JAVA程序循环嵌套过多，过于精细的查询条件，子查询间等待超时

4.程序中存在死循环引起线程死锁，导致CPU使用率达到100%

5.某些返回结果未定义处理方式，导致线程等待，不释放，CPU使用率高

数据库类

1.SGA分配不合理，需要具体情况具体分析

2.使用全表扫描

3.对于查询业务比较多的表，未建立索引，或建立的索引不合理，在索引列上使用IS NULL和IS NOT NULL

4.存在数据库死锁导致数据库连接超时或不释放。

5.存在过于复杂的计算，导致CPU、内存和IO使用率较高。

6. 数据库读写过于频繁，导致IO使用率比较高

其他问题

1.网络问题，被测试环境网络环境小于100M

2. 客户端问题等等

7.2 性能调优
性能调优是一个非常大的议题，更多的是开发人员进行，对于测试人员，可以了解一些通用的调优方法，并根据性能分析过程中发现的问题，给出一些建议。

7.2.1  性能调优目标与策略
性能优化的目标不外乎两个：1.时间性能：减小系统执行的时间；2.空间性能：减小系统占用的空间。

一般来说，性能优化也就是下面的几个策略：
用空间换时间。
用时间换空间。
简化代码。
并行处理。
总之，根据2：8原则来说，20%的代码耗了你80%的性能，找到那20%的代码，你就可以优化那80%的性能。

7.2.2  调优的原则
在应用系统的设计、开发过程中，应始终把性能放在考虑的范围内。
确定清晰明确的性能目标是关键。
必须保证调优后的程序运行正确。
性能更大程度是取决于良好的设计，调优技巧只是一个辅助手段。
调优过程是叠代渐进的过程，每次调优的结果要反馈到后续的代码开发中去。
性能调优不能以牺牲代码的可读性和维护性为代价。
7.2.3  调优的基本步骤
确定清晰的性能目标，并按优先级排列
利用科学的测试工具对应用程序进行测试，并记录测试结果。
把分布式系统拆分成组件：Web层、业务层、集成层、以及网络传输时间，分别进行调优。
有系统的科学调优
遵循一定的程序：测试性能→找出瓶颈→假设造成瓶颈的因素→测试假设是否成立→修改应用→再次测试性能
确定影响性能的因素：CPU、内存还是IO。
找出主要的瓶颈，首先解决最容易的，再重复测试。
一次修改一个瓶颈，不要对不需要的地方进行调优
提高CPU性能：更快的代码，更好的算法，减少短期生存的对象。
提高内存性能：减少或减小长期生存的对象。
提高IO性能：重新设计应用，减少IO的交互。
5.优化完成之后，进行QA测试。

6.在代码中记录优化的地方，并对旧代码进行注释。


7   性能测试报告与总结
性能测试报告是性能测试的里程碑,通过报告能展示出性能测试的最终成果,展示系统性能是否符合需求,是否有性能隐患。性能测试报告中需要阐明性能测试目标、性能测试环境、性能测试数据构造规则、性能测试策略、性能测试结果、性能测试调优说明、性能测试过程中遇到的问题和解决办法等。

性能测试工程师完成该次性能测试后，需要将测试结果进行备案，并做为下次性能测试的基线标准，具体包括性能测试结果数据、性能测试瓶颈和调优方案等。同时需要将测试过程中遇到的问题，包括代码瓶颈、配置项问题、数据问题和沟通问题，以及解决办法或解决方案，进行知识沉淀。

8   性能准入标准与建议
性能标准阈值是对指定模块程序性能的基本要求。若达不到该要求，可认为该模块程序的性能达不到上线要求。对不同类型的产品/模块/程序，性能标准阈值的表现形式和数值均有差异。根据产品/模块/程序的差异，分为如下几类：

1.Web交互型：通过浏览器与后台交互并实现各项功能

2.后台接口型：通过接口方式提供服务并实现各项功能

3.离线计算型：通过脚本定时、存储过程的方式实现后台离线计算

针对这几类应用我们给出一些性能准入测试的建议，见下表

模块类型

指标

阈值

备注

Web交互型

浏览器端操作响应时间

90%小于3秒

99%小于10秒

包括浏览器+网络+后台接口整体耗时；

网络环境按线上平均带宽预估

浏览器端渲染性能

页面渲染时间<1秒

JS响应时间<1秒

 

后台资源占用

CPU Idle 最小值>50%

CPU Idle 均值> 75%

内存占用最大峰值< 80%

内存占用均值<50%

带宽占用<30%

 

后台接口型

接口访问响应时间

90%概率小于3秒

99%概率小于10秒

包括网络+后台接口整体耗时。网络环境按线上平均带宽预估。

后台资源占用

CPU Idle 最小值>50%

CPU Idle 均值> 75%

内存占用最大峰值< 80%

内存占用均值<50%

带宽占用<30%

在正常压力范围的性能场景

离线计算型

计算耗时

耗时<运行间隔

 

资源占用

CPU Idle 最小值>25%

CPU Idle 均值> 50%

内存占用最大峰值< 80%

内存占用均值<50%"
接口测试怎么测试，有哪些内容,"[""接口测试的目的是什么"", ""接口测试的原则有哪些"", ""接口测试的检查项有哪些"", ""接口测试的方法有哪些"", ""以及注意事项有哪些""]","1.简介
1.1.接口测试的定义
接口测试是测试系统组件间接口的一种测试，主要用于检测外部系统与系统之间以及内部各个子系统之间的交互点；
接口测试的重点是要检查数据的交换，传递和控制管理过程，以及系统间的相互逻辑依赖关系等；
不管是何种接口测试，其测试都为客户端发送request请求，接着服务器会返回response报文，然后我们需要对response内容进行比对，从而来中判定接口访问是否成功，最终验证业务是否符合需求；

1.2.接口测试的目的
保证系统接口的功能正常，提高需求质量；
持续集成，提高测试效率，保证数据的准确性；
后端基础服务的架构比较复杂，接口测试让测试人员集中精力关注系统对外交互的部分，因而能够提供系统复杂度上升情况下的低成本高效率的解决方案。
2.原则
任何新增接口和修改接口都需要周知测试人员，并进行相关接口测试；
接口测试要求从白盒角度对系统的整体架构有足够了解，又要求从黑盒角度对用户场景熟悉，两者相辅相承设计测试用例；
站在用户的角度对系统接口进行全面高效持续的检测。
3.检查项
业务功能覆盖是否完整
业务规则覆盖是否完整
参数验证是否达到要求（边界、业务规则）
接口异常场景覆盖是否完整
接口覆盖率是否达到要求
代码覆盖率是否达到要求
性能指标是否满足要求
安全指标是否满足要求
4.测试内容
接口测试的覆盖面较为广泛，具体可参加下图：


5.测试方法
基于接口测试的内容，接口测试要保障业务的基本功能、性能、安全性等指标，其主要使用的测试方法包括：

功能测试	
1）基本功能性验证

2）接口验证：

输入（入参）的测试：必传和非必传、参数长度、数值类型、特殊字符&%@#等、字符串类型、数组、正常和异常、有、无和为空，以及参数组合
输出（返回）的测试：主要看正常返回的响应内容，和异常返回的响应码和提示语。
3）边界分析：在基本功能测试的基础上考虑输入输出的边界条件，这部分内容也会有重复的部分，比如业务规则的边界。


异常测试	
1）重复提交：多次重复提交，连续重复提交，特别是涉及到支付、交易金额相关

2）并发：两个以上用户操作同一场景，争夺资源、死锁等情况

3）事务测试：多个连续步骤才能完成的业务流程

4）大数据量测试：数据量较大时，能否处理正确和效率是否受影响


性能测试	
响应时间、吞吐量、并发数、服务器资源使用率、cpu、内存、io、network


安全测试	
接口的加密处理、前后端数据传输、日志信息、防刷机制。




6.注意事项
对接口测试，持续集成自动化是核心内容，通过持自动化的手段才能做到低成本高收益。目前已经实现了接口自动化，主要应用于回归阶段，后续还需要加强自动化的程度，包括但不限于下面的内容：

流程相关	在回归阶段加强接口异常场景的覆盖度，并逐步向系统测试，冒烟测试阶段延伸，最终达到全流程自动化
结果展示	更加丰富的结果展示、趋势分析，质量统计和分析等
问题定位	报错信息、日志更精准，方便问题复现与定位
结果校验	加强自动化校验能力，如数据库信息校验
代码覆盖率	不断尝试由目前的黑盒向白盒下探，提高代码覆盖率
性能需求	完善性能测试体系，通过自动化的手段监控接口性能指标是否正常"
数据库测试怎么测试，有哪些内容,"[""数据库测试的定义"", ""数据库测试的内容"", ""数据库测试的注意事项""]","1.数据库测试概述
1.1.定义
数据库测试是依据数据库设计规范对软件系统的数据库结构、数据表及其之间的数据调用关系进行的测试。无论是在web、桌面应用、客户端服务器、企业和个人业务，还是在金融、房屋、医疗、电商等领域中，都需要数据库操作。

1.2.常见数据库及操作
数据库工具	
MS-Access2010

MS SQL Server2008 r2

Oracle 10g

Oracle Financial

MySQL

DB2

常见操作	
C：创建-创建用户；

R：检索-执行检索视图操作；

U：更新-更新数据库操作；

D：删除-执行删除数据库操作。

2.数据库测试内容
2.1.数据库功能测试
需求分析产生数据库逻辑模型	
需求分析，确认数据库设计文档，并最终实现的数据库和文档相同；
数据库进行分析，测试，例如存储过程，视图，触发器，约束，规则，外键等。确保这些功能设计符合需求；
业务开发中测试	通过测试用例运行数据库，以验证该数据库功能无遗漏，对需求覆盖进行保证；
实体测试	测试表，视图，存储过程等实体，发现数据库实体设计是否充分，是否有遗漏，每个实体的内容是否全面，扩展性如何。
2.2.数据库连接测试
         1）每种类型数据库有各自的数据库连接方式，常见数据库连接访技术有ODBC，OLE DB，ADO，JDBC，ODAC等。

         2）不同的技术提供了不同的连接方式，在测试中可以根据业务的数据库连接类型，编写简单的代码测试一下数据库是否连接成功。

2.3.数据库接口测试
不同数据库有不同的连接方式，也有提供不同的接口来访问数据库，数据库接口测试主要包括：

测试点	测试内容
数据项操作

数据项的修改操作；

数据项的增加操作；

数据项的删除操作；

数据表


数据表增加满；
数据表删除空；
删除空表中的记录；
数据表并发操作；

接口测试	结合业务逻辑做关联表接口测试；
根据接口本身，采取等价类、边界值、错误猜测等方式进行测试；

2.4.数据库一致性测试
数据的一致性测试主要包括以下几个方面：

主键测试	每个表的主键进行测试，验证是否存在记录不唯一的情况，如果有，则要重新设置主键，使表中记录唯一；
表之间主外键测试	检查名称，数据类型，字段长度上的一致；
级联表测试	删除主表数据后，相应从报表数据应同时删除；
2.5.数据库容量测试
容量估算=数据库基本表的数据大小+每天数据表的数据产生量。具体的计算方法为：

记录数据量=各个字段所占字节数的总和；
表的数据量=记录数据量* 记录数；
数据库大小=各表数据量的总和；
除此之前，数据库容量还包括系统表，视图，存储过程，日志文件等实体所占的容量。

2.6.数据库性能压力测试
压力测试工具有很多，比如mysqlslap（mysql自带压测工具）、jemter、DataFactory等，其中jemter专门提供数据库压测的方法。数据库性能优化点有：

优化点	内容
样式设计	
表格结构合理

每个表是否使用适当的行格式；

功能设计	
是否为每个表使用适当的存储引擎；

应用程序是否使用适当的锁定策略；

是否正确使用缓存内存空间；
是否有适当索引来提高查询效率；

磁盘	
磁盘寻求，将数据分配到多个磁盘上来优化寻道时间；

磁盘读写，从多个磁盘并行读取进行磁盘搜索优化；

性能	
CPU周期；

内存宽带，当CPU需要的数据量超过CPU缓存容量时，主内存带宽将成为瓶颈；

2.7.数据库安全测试
数据库存储重要信息，是最宝贵的资源，因此数据库的安全性尤为重要。数据库安全测试主要从以下几个方面入手：

测试项	测试内容
数据库信息收集	
端口信息收集，检测数据库端口是否开放，使用nmap命令扫描；

版本信息收集，检查数据库的版本信息，主要使用msf，tnscmd10g两个命令扫描；

数据库管理信息收集，通过数据库客户端工具，获取数据库服务器地址、数据库用户名及密码等；

数据库密码获取	
获取数据库SID。命令上主要使用msf，Hydra，sidguess，nmap等方式爆破；还有web在线连接破解，源代码泄露，本地文件包含漏洞等方式破解；

枚举数据库账号密码。枚举爆破数据库连接账号的方式获取账号密码；

数据库提权	
mof提权；

UDF提权；

Sqlmap直接数据库提权；

启动项提权；

msf启动项提权；

身份认证漏洞及利用；

消息溢出漏洞；

获取数据库里关键信息的sql命令	
查询数据库版本；

数据库补丁情况；

查询所有用户；

查询数据库中所有表；

当前用户被激活角色；

描述数据库对象；

加密	
通信平台加密；

数据库敏感字段加密存，解密取；

数据库权限控制	
数据库权限涉及审批规则；

前端应用安全措施，如防SQl注入，防攻击等	
2.8.数据库健壮性、容错性和恢复能力测试
重启时间	数据库重新启动时间；
恢复时间	数据库从备份库恢复时间，内容是否错误；
下线时间	数据库下线时间；
主库切换从库时间	主库快速切换到从库耗时，是否发生错误；
数据丢失应急措施	数据丢失后，是否能从内存，日志获取丢失数据；
2.9.数据库备份测试
是否备份	检查数据库是否设计备份库，同步脚本是否正确；
备份时间	数据库异步备份时间，是否立即备份，是否在业务高峰期；
备份是否正确	备份的数据是否正确；
备份中断	备份中断后，数据是否丢失，是否能恢复；
备份库失效的应急措施	备份库失效时，应急降级措施是什么；
3.数据库测试注意事项
很多数据库测试比较简单，导致没有发现真正的问题，以下列举了数据库测试中容易疏忽的点：

不进行数据库测试，很多业务不测试数据库；
不测试数据库Schema，不对数据库模式/架构测试；
不在生产环境测试，测试环境测试完成后，应该在生产环境进行回归，但是要事前做好数据清除准备；
不准备好数据库就进行测试，必须在测试前准备环境，而非测试后；
准备了数据库确不对其检查就进行测试，数据库准备好后，要检查数据库状态；
不测试创建脚本；
不测试外键，数据库应该拥有各种外键；
不测试默认值，数据库应该定义一些合理的默认值；
不测试约束，数据库应该拥有多个约束。"
用户体验测试怎么测试，有哪些内容,"[""用户体验测试的定义"", ""用户体验测试的目的"", ""用户体验测试的要求"", ""用户体验测试的内容"", ""用户体验测试的方法""]","1.简介
1.1.用户体验定义
用户体验，英文叫做user experience。是指用户访问一个网站或者使用一个产品时的全部体验。包括用户的印象和感觉，是否成功，是否享受，是否还想再来使用，是否能够忍受的问题等；
用户体验是用户在使用产品时所建立起来的心理感受。
用户体验的共性是能够通过良好的设计来实现的。 如今用户成了强势的群体，他们不再满足于使用的软件能实现其需要的功能，更追求一种使用过程中的良好的心理感受，产品不好，用户就会抛弃产品；
总结一点：牛逼的功能千篇一律， 好的用户体验万里挑一。
1.2.用户体验测试定义
用户体验测试是测试人员在将产品交付客户之前处于用户角度进行的一系列体验使用，测试人员站在用户角度进行体验使用，最终目的就是验证我们的产品是否符合用户习惯。如：界面是否友好（吸引用户眼球，给其眼前一亮惊为天人的体验，用了还想再用）、操作是否流畅、功能是否达到用户使用要求等。

1.3.用户体验测试目的
满足用户需求，超出用户期望；
判定产品是否能让用户快速的接受和使用；
是否会不符合用户的习惯；
是否让用户对产品产生抗拒。
2.原则
用户体验测试原则是适应用户而不是改变用户；
用户体验测试介入时间要尽可能早，建议是当页面demo定稿时开始用户体验测试。当系统将要发布时，再进行用户体验测试，会有项目延期风险；
功能测试主流程正常流程后，用户体验测试更关注的是收集用户的操作习惯和使用感受。
考虑实际收益，用户体验测试的设计需要慎之又慎，他需要对测试的目的、介入时间、测试的周期、场景、人员的选型都要做出深入的分析和界定
3.要求
用户端（C端）产品必须进行用户体验测试；
作业端（B端）产品根据用户流量选择非强制的进行用户体验测试。
4.测试内容
以主观的普通消费者的角度去感知产品或服务的舒适、有用、易用、友好亲切程度。 通过不同个体、独立空间和非经验的统计复用方式去有效评价产品的体验特性提出修改意见提升产品的潜在客户满意度。

分类	检查点	说明
无效引导	是否有空数据界面设计，引导用户去执行操作；	
是否滥用用户引导；	
是否有不可点击的效果，如：你的按钮此时处于不可用状态，那么一定要灰掉，或者拿掉按钮，否则会给用户误导；	
页面易用性	菜单层次是否太深；	
交互流程分支是否太多；	
相关的选项是否离得很远；	
一次是否载入太多的数据；	
界面中按钮可点击范围是否适中；	
网站页面是否过长，内容过多，引起浏览者视觉疲劳；	
链接是否用标准的形式呈现，网站有标准的链接表现形式，比如“更多”用中文，不要用more或者标点符号代替；


设计易用性	标签页是否跟内容没有从属关系，当切换标签的时候，内容跟着切换；	
操作应该有主次从属关系；	
是否定义Back的逻辑。涉及软硬件交互时，Back键应具体定义；	
是否有横屏模式的设计，应用一般需要支持横屏模式，即自适应设计；	
导航是否友好，不知如何返回上一页，不知当前页面在哪个栏目下；


过于复杂的验证码，不清晰的验证码	
页面性能	
页面加载，下载时间是否过长，是否在合理的加载范围内；


是否不限制使用flash及图片，造成页面文件超大，占用浏览器cpu资源；	
页面是否有过多新窗口，大量占用计算机资源；	
页面信息	是否存在过期信息，页面存在长期不更新的过期信息，引起反感；	
是否有死链接或者链接错误；	
是否有页面安全信息，例如用户信息暴露页面上；	
是否有恶意插件，恶意弹出窗口；	
过多运用新技术，新技术通常是少数人掌握，不适用90%用户，容易引起不适感；


5.测试方法
     1）体验更多的产品，网站或者APP。不只是国内的产品，国外的产品也要多体验，从产品的设计和功能的布局上为什么别人要这么做？多思考总结；

     2）用户反馈。多看用户对产品的体验反馈、建议等，反馈渠道有：AppStore及其他安卓市场看产品的评论，从评论中提取到关键信息；

     3）从数据统计上观察用户的操作习惯，喜爱程度等，主要从以下几个方面去总结：

分析APP安装情况，活跃情况，都使用了哪些手机型号，手机型号的使用排名，crash情况等。提高APP的稳定性，从而提高用户体验。
跟踪页面的访问，按钮的点击情况，从而分析数据，优化页面。

通过公司的数据中心部门，进行数据的挖掘和分析，做出更多优化。"
用户界面测试怎么测试，有哪些内容,"[""用户界面测试的定义"", ""用户界面测试的目的"", ""用户界面测试的要求"", ""用户界面测试的内容"", ""用户界面测试的方法""]","1.简介
1.1.用户界面测试定义
界面测试（简称UI测试)，是指测试人员测试用户界面的功能模块的布局是否合理、整体风格是否一致、各个控件的放置位置是否符合客户使用习惯，此外还要测试界面操作便捷性、导航简单易懂性，页面元素的可用性，界面中文字是否正确，命名是否统一，页面是否美观，文字、图片组合是否完美等。

1.2.用户界面测试目的
通过浏览测试对象可正确反映业务的功能和需求，这种浏览包括窗口与窗口之间、字段与字段之间的浏览，以及各种访问方法 （Tab 健、鼠标移动和快捷键）的使用；

保障窗口的对象和特征（例如：菜单、大小、位置、状态和中心）都符合标准。

2.原则
异常用户界面零容忍；

3.要求
所有用户端项目进行用户界面测试；

4.测试内容
4.1.静态测试内容
页面设计	所有页面设计风格需保持一致；	

页面结构设计布局合理，主题和页面排版合理，无页面变形，无样式丢失，背景显示正常，背景颜色与字体颜色和背景色协调，信息显示完整。界面的线条一致，每个界面中线条对齐且一致；	

页面文字格式统一，颜色统一，无错别字	

页面标点符号格式统一，所有字段后若存在冒号，需查看冒号为统一的中文冒号或英文冒号	

页面所有的列表页标题字是不会折行，标题字显示需统一，统一居中，或统一居左，或统一居右；	

页面中的提示说明叙述需简明，尽量简化，并且字体显示格式一致，颜色统一；	

页面按钮格式及颜色需一致；	

页面界面所有的展示图片需样式一致，图片需显示清晰；	

页面存在必填项时，需有必填项标志；	
兼容性	页面能兼容不同的浏览器，检查页面在不同浏览器下不会发生异常；	

页面在不同分辨率，窗口大小中展示无变形；	
导航菜单	导航菜单布局合理，导航直观，且易于用户操作；	

页面导航与页面结构、菜单、连接页面的风格一致；	

页面导航易于导航且直观，若有导航帮助，导航帮助需准确直观，当存在多级菜单时，菜单图标需做区分，便于用户导航，用户当前操作页面的导航菜单需高亮显示提示用户，当前操作页面的导航路径需要正确显示；	

页面数据需要搜索栏时，搜索栏需有效直观；	
表格	页面存在表格时，表格宽度需足够，并显示完整。当表格呈现内容过多时，需有效提示便于用户操作查看，比如增加更多按钮，点击可查看更多内容，或有滚动条显示；	

页面存在表格时，页面表格中的内容超过最长字符限制用...显示，鼠标放上去后显示全部内容；	

页面存在列表时，当前列表若无数据时，有相关提示；	
其他	
页面能读取需求中需要显示数据源，且数据正确读出，并完整显示；



页面中公司图标，所有权归属，系统图标显示正确，图标在不同浏览器及分辨率均能正常显。	
4.2.动态测试内容
操作友好提示	页面操作友好易用；	

页面导航菜单链接有效，点击后不会出现404，500报错；	

页面所有的输入框都可以进行校验，超出限制时需有提示；	

页面所有的按钮点击后有响应；	

页面所有的下拉框点击后有响应，下拉框数据显示正常且完整，取值正确，不会溢出框外；	

必填项未填写时需有提示框弹出，显示提示语：xxx必填；	
友好动态显示	
在对后台进行数据添加时，查看前台页面是需随后台数据一起变化，页面的数据不会溢出框外。后台增加表格内容时，根据需求出现折行或者滚动条，显示完整数据，页面不会变形；



页面列表存在多条数据时，需有显示滚动或翻页功能使用户可查看完整数据；	
搜索	页面存在搜索项时，设置查询条件，不点击查询按钮，翻页时不会改变用户行为，不会自动根据查询条件显示数据；	

页面存在搜索项时，设置查询条件，点击查询，翻页时显示符合查询条件的数据；	
输入/输出	页面输入项输入异常字符时需处理，不会出现系统报错，输入中文字符时需处理不会出现乱码或出错。输入非法数据时，需提示；	

输入项输入正确数据时，输入和输出需保持一致；


页面操作	
在页面点【退出】按钮，成功退出到登录界面；



点击删除按钮时需有提示框弹出提示用户确定删除数据，避免误操作丢失数据；	

当存在批量删除功能时，不选择任何数据，点击删除按钮需有提示；	

取消操作可执行成功；	

成功操作可根据需求在规定的时间及范围内生效；	

缩放窗体，窗体上的控件应随窗体的大小变化而变化。	
4.3.文件上传
文件格式检查	上传文件大小在需求限制范围内可成功上传；	

上传文件大小超出需求限制范围需提示超出限制，不可成功上传；	

上传文件数量在需求限制范围内可成功上传；	

上传文件数量超出需求限制范围需提示超出限制，不可成功上传；	

上传文件格式在需求限制范围内可成功上传；	

上传文件格式超出需求限制范围需提示超出限制，不可成功上传；



上传文件无效文件时，举例：大小为0kb的文件，不可成功上传；


兼容性显示	
上传成功的文件名称显示正确无异常；



在不同浏览器上传符合要求的文件均可成功上传；


上传操作	根据需求，若上传路径支持手动输入时，输入正确的文件路径可成功上传；	

根据需求，若上传路径支持手动输入时，输入错误的文件路径不可成功上传；	

已被打开的文件上传，根据用户需求确定能否成功上传；



上传文件的过程中，若文件上传未完成时，可取消上传操作；	

若上传文件成功后，页面存在数据保存或提交按钮时，未点击保存或提交按钮，上传文件则不显示在页面中；	

上传文件允许多个时，再次上传文件，上传页面不可显示上次文件的名称，需刷新上传页面；	

上传为空时，点击上传按钮，需弹出提示信息；



已损坏文件上传时，需提示，不可成功上传；



文件存储位置路径深度测试，当超出最大深度，需提示，上传文件不可成功；	

文件目标服务器已满时，上传文件需提示，文件上传不成功。


4.4.文件下载
可以下载成功，下载文件和上传文件大小，文件名称，格式一致；	
下载文件打开内容正确，和上传文件内容一致，无乱码，无异常；	
不同浏览器都可下载下载成功，文件大小，文件名称，格式，内容一致；	
下载未完成时，可取消下载操作；	
可下载文件后台被删除时，点击下载需提示，根据用户需求而定。	
5.测试方法
5.1.静态测试
对于用户界面的布局，风格，字体，图片等与显示相关的部分测试应该采用静态测试，比如点检表测试，即将测试必须通过的项用点检表一条一条列举出，然后通过观察确保每项是否通过。

5.2.动态测试
对用户界面中各个类别的控件应该采用动态测试，即编写测试用例或者点检表，对每个按钮的响应情况进行测试，是否符合概要设计所规定的条件，还可以对用户界面在不同环境下的显示情况进行测试。"
移动端UI自动化测试怎么测试，有哪些内容,"[""UI自动化解决的问题有哪些"", ""UI自动化的应用场景"", ""UI自动化用例编写规范"", ""UI测试的落地指标""]","规范UI自动化测试，明确UI自动化测试使用场景和应用规范

1.UI自动化解决的问题
   1、重复性的功能测试及验证

   2、避免疲惫操作时的人为测试遗漏

   3、通过UI自动化操作获取其他测试数据的能力

   4、核心功能验证

2.UI自动化应用场景
2.1.UI自动化程序的执行方式：
   a、手工执行

   b、定时任务执行

   c、持续集成(CI)执行  

2.2.应用场景：
冒烟测试

研发自测时，可运行自动化冒烟用例，保障核心功能正确；
回归测试	版本迭代过程中，旧功能占据了checkList的80%，优先将高优先级的Case、耗时的Case优先接入到Ui自动化场景中，可以大量节约回归测试时间；
每日巡检	 对于商机/埋点等重要模块的每日自动化巡检；
3.怎样选择适合团队的测试方案
 考虑以下几个方面：

平台支持	自动化技术平台支撑
稳定性	考虑业务稳定性，测试用例的稳定性
维护成本	业务是否经常变更，变更时维护耗时
可扩展性	业务扩展时，测试用例是否可扩展
4.UI自动化用例编写规范
分层设计和PageObjects模式

a.按业务特点，分功能模块或入口展示页面，划分case结构。

b.case中不准许出现页面元素信息，所有页面元素的封装和业务逻辑的封装要写在page层中

c.配置文件、基础函数和case文件分离保存

命名及注释规范

a.基础函数封装注释

b.变量命名遵守驼峰命名法

c.增加case说明

case设计规范

a.用例完整结构：查找，操作，断言

b.case级别设置

c.用例解耦

d.不使用绝对坐标点击

进阶设计

a.兼容资源混淆   

b.权限弹框处理     

c.失败重试机制，提高用例稳定性     

d.日志失败截图,协助快速定位问题     

e.数据统计     

f.CI平台对接     

移动端UI测试是线下本机环境执行和操作手机，可以搭建一个线上的公共平台来选择和触发UI的执行，作为一种常规自动化测试类型嵌入到敏捷测试流程中

5.数据安全规范
     禁止采用线上真实城市进行商机模块的UI自动化测试，采用线上虚拟城市进行商机模块的UI自动化测试，避免污染线上数据。

6.UI测试的落地指标
6.1.用例执行通过率（稳定性）
    通过率的定义：（成功数 / (成功+失败+跳过数) ）* 100%

6.2.核心用例覆盖率
    覆盖率定义：已实现自动化用例数 / 功能测试核心用例总数 * 100%"
移动端稳定性测试怎么测试，有哪些内容,"[""移动端稳定性测试的定义"", ""移动端稳定性测试目标是什么"", ""为什么要做稳定性测试"", ""移动端稳定性测试的收益"", ""移动端稳定性测试的方法""]","1.稳定性测试简介
1.1.稳定性测试定义
稳定性测试是在保证功能完整正确的前提下，必不可少的一项测试内容，通过对软件稳定性的测试可以观察在一个运行周期内、一定的压力条件下，软件的出错机率、性能劣化趋势等，进而大大减少软件上线后的崩溃卡死等现象，为软件的逐步优化提供方向及验证。

1.2.稳定性测试目标
发现贝壳移动端稳定性问题（Crash、ANR、闪退等）和性能问题（电量、启动加载、cup、内存资源等），提高APP的稳定性、健壮性。

2.为什么做
线上crash、anr等异常数量，很多场景手工不能触发
用户使用过程app卡顿或者闪退会造成CTR降低，用户流失等
线上用户的机型、sdk变化多样，而测试结果很难手工都去回归一遍
而且线上版本很多，老版本的回归测试也很耗时
3.收益
通过稳定性测试提前发现问题，减低线上crash、anr率
通过稳定性测试并配合性能测试，降低卡顿，提升用户体验
通过自动化，线下回归测试不同手机(屏幕、机型、sdk、网络等)
通过自动化和稳定性，保证老版本app的稳定性怎么做
4.稳定性测试方法
4.1.Android:原生money
4.1.1Mony的一般使用步骤
         1）先确认设备（模拟器或者真机）是否连接成功。命令：adb devices，出现设备号则表示连接成功
         2）确定待测应用的包名。参考如下
               打开待测应用后，输入命令：adb shell dumpsys activity | find ""mFocusedActivity"",得到包名
         3）执行mony测试命令：adb shell mony -p com.xxx.xxx -v -v 200

4.1. 2Mony的常见命令
-help 查看mony用法，其命令格式：adb shell mony [限制命令 参数值][事件名 百分比] 事件次数

-p <allowed-package-name>	用于约束限制，用此参数指定一个或多个包。指定包之后，Mony将只允许系统启动指定的APP。如果不指定包，Mony将允许系统启动设备中的所有APP。指定多个包，使用多个-p，一个-p后面接一个包名。
-v	日志级别 Level0
-v -v	日志级别 Level 1
-v -v -v	日志级别 Level 2
-s	用于指定伪随机数生成器的seed值，如果seed相同，则两次Mony测试所产生的事件序列也相同的。一般测试过程中出现崩溃，可以通过seed值来完成复现。
--throttle <毫秒>	
用于指定用户操作（即事件）间的时延，单位是毫秒

--randomize-throttle	用这个参数必须与--throttle 绑定使用。用于在事件之间插入随机延迟，随机延迟范围为0到throttle设置的时间，单位为毫秒；
--pkg-whitelist-file 白名单	后面接txt文件，指定白名单参数后只测试白名单的应用程序，黑白名单命令不能同时使用
--pkg-blacklist-file 黑名单	后面接txt文件，指定黑名单参数后，避开黑名单的应用程序，黑白名单命令不能同时使用
4.2.iOS:Fastmony
4.2.1 Fastmony简介
Fastmony是zhangzhao大神去年（2017年）开源的一款iOS应用Mony测试工具，基于 XCTestWD、swiftmony 二次开发，实现无需插桩的 iOS mony 自动化工具 fastmony。优点如下：

无需插桩！
高效率，每秒4-5个action！
轻量极简！
4.2.2 Fastmony准备工作
          1）Xcode；

          2）iOS 第三方库管理工具-Carthage；

          3）iOS开发者证书文件；

          4）Fastmony工程代码从github上下载

4.2.3 配置过程
1）下载工程所依赖的包	 $ carthage update	
2）配置XCTestWD项目工程	
1. 双击子目录XCTestWD中的XCTestWD.xcodeproj文件打开项目工程

2. 修改XCTestWDUITests所需要的产品ID如com.xxx.XCTestWD.XCTestWD

3. xxx为证书所定义的产品ID前缀一般是公司或者企业的域名


3）修改签名账号（个人开发Team与团队证书设置是不一样的）	
1. 修改XCTestWD Code Signing为相应的账号

2. 修改XCTestWD Code Signing为相应的Team账号

3. 同a1过程修改XCTestWDUITests的Code Signing账号


4）添加XCTestWDMony.swfit到Server目录	
右键点击server目录，然选择""Add Files to ""XCTestWD""

选择【XCTestWDMony.swift】文件添加到Server


5）清空XCTestWD的Objective-C Bridging Header选项中的值	
清空Objective-C Bridging Header选项中的值

清空Objective-C Bridging Header选项


6）修改XCTestWD项目工程代码中的配置值	
1. 修改运行模式，将XCTestWDRunner.swift文件中的serverMode设置为false，修改为false后，这样Mony就可以直接从Xcode中运行，不需要再使用额外的命令了

2. 修改XCTestWDMony.swift文件中的bundleID为被测App的值。改为自己App的实际BundleID，如何查找被测App的BundleID请百度一下这里不再敖述

3. 修改Mony.swift文件中的elapsedTime值确定你需要运行多长时间的Mony，注意单位是秒


4.2.4 执行Mony测试 
          1）手工执行

1. 选择要执行的手机与Target注意App事先安装到被测试手机上，手机已经插入Mac电脑USB接口，从下拉列表中选择XCTestWDUITests与手机
2. 开始执行Mony，选择【Product】–【Test】执行Mony测试

   2）自动化执行

        1. 命令行执行：xcodebuild -project WebDriverAgent.xcodeproj -scheme WebDriverAgentRunner -destination id='udid' test

        2. 可以结合jenkins或其他定时脚本做CI执行，达到持续运行的效果

5.问题上报-海神平台（ 贝壳Bugly地址：http://bugly..com/）
腾讯 Bugly，是腾讯公司为移动开发者开放的服务之一，面向移动开发者提供专业的 Crash 监控、崩溃分析等质量跟踪服务。
针对移动应用，腾讯 Bugly 提供了专业的 Crash、Android ANR ( application not response)、iOS 卡顿监控和解决方案。
移动开发者 ( Android / iOS ) 可以通过监控，快速发现用户在使用过程中出现的 Crash (崩溃)、Android ANR 和 iOS 卡顿，并根据上报的信息快速定位和解决问题。"
线上压测怎么测试，有哪些内容,"[""压测是否需要报备"", ""压测怎么报备"", ""线上报备的原则是什么"", ""压测的时间一般是什么时候"", ""压测的数据应该怎么准备""]","1.压测报备
 线上压测前必须发送压测申请邮件进行报备（至少提前一个工作日），通知相关责任人，如运维、DBA、研发、运营、测试团队，服务依赖方等，申请邮件须经质量部总监审批；各团队必须有指定人员现场支持，出现紧急情况便于及时处理；压测报备申请邮件格式如下：

【XX系统/服务】线上压测/演练报备

时间

XXXX年XX月XX日XX时XX分

目的

如压测预解决的问题、预了解的测试内容的承压能力、预了解当前环境支持的最大并发

环境

预生产环境、线上环境等

数据准备	虚拟城市账号
演练场景	故障注入的具体场景
类型

接口、业务流、接口说明、接口列表、业务说明

指标

并发量，响应时间、吞吐量等

方案



系统/服务架构图



系统/服务部署机器



负责人

压测整体负责人

干系人

运维、开发、测试、上下游依赖等值班人

其它说明



2.压测预案
对业务系统压测前，要和开发、运维团队做好预案，比如系统宕机后，怎么恢复；如果压测涉及到写库操作，必须提前做好数据清理方案；如果没有特殊需求，建议进行IP方式而非域名方式压测；

3.压测时间
线上压测核心原则，不能影响真实用户的使用，压测时间必须选择每天业务最低峰，比如，在0:00-6:00之间（以各业务实际低峰作业期为准），这样对系统的影响最小；压测前，需要梳理高峰业务场景，可以按照问题描述模板来梳理业务场景，例如：

场景1：2019年6月29日00:00~01:00，共计xx万个用户同时操作xx。

场景2：每天xx时间段，xx系统会轮询调用xx服务xxxx次。

4.数据准备
线上压测不能使用真实的用户数据，可以利用线上虚拟城市，提前准备业务数据。如果涉及到一些金钱的操作，比如发短信，提前把开关关闭。

5.压测执行
(一)  起始并发一定要小一些，防止系统性能不好，直接崩溃；

(二)  压测时间不宜过长，除特殊场景外，一般3-5分钟即可；

(三)  压测时系统要做系统全链路监控，一旦出现异常情况，如机器负载高、报错率上升等，应立即停止压测，排查问题；

6.数据清理
压测结束后，要根据提前制定的数据清理方案，将压测产生的垃圾数据清理掉，压测负责人负责确认垃圾数据是否完全清理，并确认压测后服务整体运行正常。"
用例设计的规范是什么,"[""用例设计的规范"", ""用例设计的依据"", ""用例设计的原则"", ""用例设计的标准"", ""用例设计的步骤"", ""用例优先级划分"", ""用例的维护""]","1.引言
 1.1.目的
为测试用例的质量负责，使测试工作能有序、合理化的进行，从而提高实施测试时对所测产品、系统或者模块的测试质量，也是作为各测试人员在设计用例时的一种规范，使之设计的用例能有效的被管理。 

1.2.适用范围
本文档适用于测试人员。

2. 用例规范 
2.1.用途 
ü  指导测试工作有序进行，使实施测试的数据有据可依；

ü  确保所实现的功能与客户预期的需求相符合；

ü  完善软件不同版本之间的重复性测试；

ü  跟踪测试进度，确定测试重点；

ü  评估测试结果的度量标准；

ü  增强软件的可信任度；

ü  分析缺陷的标准；

2.2.设计依据 
ü  需求说明书； 

ü  项目测试需求功能点； 

ü  所属行业的业务知识掌握程度；

ü  测试工程师本人的理解程度（个人经验）； 

2.3.用例内容
用例编号：  唯一标识，与需求编号对应，为多对一关系  

模块：模块名称 

子模块：  子模块名称

用例标题：  对测试项简短的描述 

用例级别：  确定用例执行的级别

前提条件：  执行用例时需要的预置条件 

操作步骤：  执行该动作需要完成的操作 

预期结果：  执行完该动作后程序的表现结果  

实际结果：  实际输出的结果 

问题描述：  执行该用例出现后系统显示的错误  

验证结果：  该测试用例是否执行通过 

BUG编号： 填写bug库中对应此用例的BUG编号 

测试执行者：按照该用例执行测试的人员  

 2.4.编写原则
2.4.1.系统性
对系统业务流程要完整说明整个系统的业务需求、系统由几个子系统组成以及它们之间的关系；对模块业务流程要说明子系统内部功能、重点功能以及它们之间的关系； 

2.4.2.连贯性
对系统业务流程要说明各个子系统之间是如何连接在一起，若需要接口，各子系统之间是否有正确的接口，若是依靠页面链接，则页面的链接是否正确；对模块业务流程要说明同级模块以及上下级模块是如何构成一个子系统，其内部功能接口是否连贯；  

2.4.3.全面性
应尽可能覆盖各种路径、尽可能覆盖各个业务点，并要考虑跨年、跨月的数据以及大数据量并发测试的准备；

2.4.4.正确性
输入界面后的数据应与测试文档所记录的数据一致，而预期结果也应与测试数据发生的业务吻合；  

2.4.5.符合正常业务规则
测试数据要符合用户实际工作中的业务流程，同时也要兼顾各种业务的变化以及当前该业务行业的法律、法规；人名、地名、电话号码等应具有模拟功能，符合一般的命名惯例；不允许出现与知名人士、小说中人物名等雷同情况；

2.4.6.可操作性
测试用例中要写清楚测试的操作步骤，以及不同的操作步骤相对应的测试结果；

2.5.编写标准 
ü  测试案例编写应该制订统一的模板进行，并约定模板的使用方法； 

ü  测试案例编写应当根据项目实际情况编写测试案例编写手册，包括案例编号规则、案例编写方法、案例编写内容、案例维护等内容； 案例编写应根据手册中约定的编写方法、内容等进行编写； 

ü  案例编写要步骤明确，输入输出要素清晰，并且与需求和缺陷相对应； 

ü  案例编写应严格根据需求规格说明书及测试需求功能分析点进行，要求覆盖全部需求功能点； 

ü  注重案例的可复用性，即在以后相似系统的测试过程中可以重复使用，减少测试设计工作量。 

2.6.设计步骤 
2.6.1.测试需求分析
从软件需求分析文档中，找出待测软件/模块的需求，通过自己的分析、理解，整理成为测试需求，要清楚被测对象具体包含哪些功能点； 

2.6.2.业务流程分析
对所在行业的业务知识要熟悉，然后对被测软件/模块的业务流程要进行全盘的整理出来（可画简单的流程图作为参考），主要包含该业务流程的主流程、备选流程、数据流向、关键判断条件以及完成该操作的非必要条件； 

2.6.3.测试用例设计
测试用例设计的类型主要包括功能测试、边界测试、异常测试、性能测试、压力测试等，在设计用例时要尽量考虑边界、异常等情况； 

2.6.4.测试用例评审
由测试用例设计者发起，参加的人员需包括测试负责人、产品人员、开发人员及其他相关的测试人员；

2.6.5.测试用例完善
测试用例编写完成之后需不断完善，软件产品新增功能或更新需求后，测试用例必须配套修改更新；在测试过程中发现设计测试用例时考虑不周，需要对测试用例进行修改完善；在软件交付使用后客户反馈的软件缺陷，而缺陷又是因测试用例存在漏洞造成，也需要对测试用例进行完善； 

2.7.用例级别划分 
目的： 保证所设计的用例在实施测试时真正起到指导作用，突出测试的重点，可以有针对性的实施测试；      

对测试用例进行优先级的划分，一般需要从三个方面考虑：

核心：确保系统基本功能及主要功能的测试用例；

重要：确保系统功能的完善方面的测试用例；

次要：关于用户体验，输入输出的验证以及其他较少使用或辅助功能的测试用例；

对应的，我们对测试用例的执行分为三个级别：高、中、低；

高（优先执行）：即关键路径的测试用例，包括最常执行的功能、基本流程的输入以及界面数据有效性校验作为高级别的测试用例；若该级别的测试用例完全执行通过，则表示该软件功能渐趋稳定； 

中（次级执行）：即可接收级测试的用例，包括不常执行的功能、异常流程的输入、边界值以及异常数据的输入作为中等级别的测试用例；若该级别的测试用例完全执行通过，则表示该软件可以进行发布了； 

低（最后执行）：即建议执行的测试用例，也就是说该级别的测试用例不是不重要，而是该级别的用例在整个项目的生命周期内不是常常被运行，包括：GUI、界面显示、错误信息提示不统一、可用性、压力和性能测试等。  

备注：对已有的用例级别说明，包括A-正常流程测试、B-异常流程测试、C-页面元素正常输入测试、D-页面元素异常输入测试、E-页面元素显示测试，可具体归类如下（仅供参考）： 

高（3）：A-正常流程测试、C-页面元素正常输入测试； 

中（2）：B-异常流程测试、D-页面元素异常输入测试； 

低（1）：E-页面元素显示测试； 

2.8.用例的维护 
2.8.1.删除过时的测试用例
因为需求的改变等原因可能会使一个基线测试用例不再适合被测系统，那么这些测试用例就会过时，需要对这些测试用例进行及时的删除，在删除过程中，不能够将整行的测试用例删除，应该将要删除的测试用例整行置灰，并将该行的用例计数器清为空；当整个功能模块需要删除时，则将整个SHEET状态置灰，并将用例计数器清空； 

2.8.2.修改的测试用例
随着软件项目的进展，测试需求可能会有部分变更，甚至大范围的变更，这个时候我们就会根据需求的变化相应的对测试用例进行维护，修改已经不符合目前需求的内容，并在备注栏中加以说明； 

2.8.3.删除冗余的测试用例
如果存在两个或更多测试用例对一组相同的输入和输入进行测试，则需要对其进行删除，只需留下其中的一个； 

2.8.4.增添新的测试用例
对新增的功能、在评审过程及测试过程中发现缺少测试用例或者系统出现BUG但是没有与之对应的测试用例，需要按照测试用例的设计标准进行增添，增加测试用例时，需要在相应功能模块的最下方插入新增的测试用例，并在备注栏中加以说明。"
专项测试集都包含哪些专项测试,"[""专项测试包含哪些内容"", ""专项测试"", ""目前金融支付有多少种专项测试""]",包含code diff测试，cookie测试，兼容性测试，功能测试，安全测试，性能测试，接口测试，数据库测试，用户体验测试，用户界面测试，移动端UI自动化，移动端稳定性测试，线上压测
缺陷管理规范集都包含哪些内容,,包含复盘机制，线上问题管理规范，线下缺陷管理规范
复盘管理机制是什么，应该怎么做,"[""月度复盘时间"", ""月度复盘参与人"", ""月度复盘内容""]","一、月度复盘时间
时间：每个月的第四周的周五下午5点
二、月度复盘参与人
参与人：建议金融支付全员，往前推近三个月「含本月」系统出现过问题的，产品、研发、测试，必须参加
分享人：当月问题的研发+测试
典型问题分享人：核算单元负责人指定研发去分享
三、月度复盘内容
当月问题，重点讲述问题产生的根因，以及如何规避，可参考的后续操作sop
挑一个典型问题，做最后环节的分享，典型问题挑选的原则：轮询各个核算单元|问题比较有代表性"
线上问题管理规范是什么,"[""线上问题分级"", ""线上问题分类"", ""线上问题来源"", ""线上问题状态"", ""线上问题填写"", ""线上问题复盘""]","1.线上问题分级
1.1.P0-致命线上问题
不能完全满足系统要求，系统停止运行，系统的重要部件无法运行，系统崩溃或者挂起等导致系统不能正常运行。修改优先级为最高，该级别问题需要立即修改。例如:

1.系统崩溃；

2.导致程序重启,死机或非法退出；

3.死循环；

4.数据丢失或异常；

5.数据通讯错误。

6.硬件故障，系统悬挂

1.2.P1-严重线上问题
严重地影响系统要求或基本功能的实现，且没有更正办法（重新安装或重新启动该软件不属于更正办法）。使系统不稳定、或破坏数据、或产生错误结果，或部分功能无法执行，而且是常规操作中经常发生或非常规操作中不可避免的主要问题，系统无法满足主要的业务要求，性能、功能或可用性严重降低。 修改优先级为高，该级别需要程序员尽快修改。 例如：

1.功能不符合用户需求；

2.数据计算错误；

3.业务流程错误；

4.程序接口错误；

5.因错误操作迫使程序中断；

6.系统可被执行，但操作功能无法执行（含指令）；

7.功能项的某些项目（选项）使用无效（对系统非致命的）；

8.功能实现不完整，如删除时没有考虑数据关联；

9.功能的实现不正确，如在系统实现的界面上，一些可接受输入的控件点击后无作用，对数据库的操作不能正确实现。

1.3.P2-一般线上问题
系统可以满足业务要求，系统性能或响应时间变慢、产生错误的中间结果但不影响最终结果等影响有限的问题。 修改优先级为中，该级别需要程序员修改。 例如：

1.数据长度不一致

2.内容或格式错误

3.响应时间较慢

4.功能性建议

5.提示信息不太准确

6.操作界面错误（包括数据窗口内列名定义、含义是否一致）；

7.简单的输入限制未放在前台进行控制；

8.虽然正确性不受影响，但系统性能和响应时间受到影响；

9.不能定位焦点或定位有误，影响功能实现；

10.增删改功能，在本界面不能实现，但在另一界面可以补充实现。

1.4.P3-低级线上问题
使操作者不方便或操作麻烦，但它不影响执行工作功能或重要功能。界面拼写错误或用户使用不方便等小问题或需要完善的问题。修改优先级为低，该级别需要程序员修改或不修改。

1.界面不规范；

2.辅助说明描述不清楚；

3.输入输出不规范；

4.长时间操作未给用户提示；

5.提示窗口文字未采用行业术语；

6.可输入区域和只读区域没有明显的区分标志；

7.必填项与非必填项应加以区别；

8.滚动条无效；

9.键盘支持不好，如在可输入多行的字段中，不支持回车换行；

10.界面不能及时刷新，影响功能实现。

1.5.P4-建议线上问题
希望提出的建议以及建议进行但不强制进行的修改。不会给发布的准确性或可用性带来任何严重影响。 修改优先级为低，该级别需要程序员修改或不修改。 例如：

1.各种提示框信息使用不统一,未采用行业术语；

2.界面显示或描述建议；

3.光标跳转设置不好，鼠标（光标）定位错误；

4.其他建议性问题。

2.线上问题分类
2.1.需求缺陷
需求文档中缺少相应描述，需求变更；

2.2.功能逻辑
功能上的错误性线上问题；

2.3.安全漏洞
XSS注入，SQL注入，跨域等问题；

2.4.性能问题
服务端性能：响应时间过长，CPU过高，GC频繁，没有分页，没有缓存等；

客户端性能：包大小，冷启动时间，流量，内存泄漏，加载时间，耗电量，页面加载等；

2.5.环境问题
被测服务环境不稳定，测试配置等；

2.6.数据问题
错误数据，不完整数据等；

2.7.交互UI
页面设计与交互UI不符等；

2.8.外部问题
依赖的外部系统引入的问题等；

2.9.技术缺陷
技术方案考虑不足，代码异常处理不足，前后端接口定义不一致，对边界、异常场景考虑不全等；

2.10.运营配置
链接、数据配置错误等；

2.11.兼容性问题
3.线上问题来源
3.1.内部反馈
公司内部人员反馈，包括不限于研发人员、测试人员、产品经理、运营、经纪人等；

3.2.用户反馈
外网收集，亲朋友好反馈等；

3.3 线上巡检
线上巡检发现，参看：《质量域_规范_线上巡检》；

3.4 线上回归
上线后，各角色进行线上回归发现；

3.5.功能自查
线上功能自查；

3.6.灰度发布
在灰度发布过程中发现的问题，包括不限于线上功能测试，灰度报警，灰度用户反馈；

3.7.监控报警
各个角色从报警发现的线上问题；

4.线上问题状态
4.1.待排查
线上问题新建时的初始状态，由线上问题发现人提给研发负责人；

4.2.开发中
研发负责人开始处理线上问题，由研发负责人变更线上问题状态，从待排查-->开发中；

4.3.待测试
线上问题经过研发负责人修复后，由研发负责人变更线上问题状态，从开发中→待测试；

4.4.无效
线上问题经过测试负责人验证后，发现问题无效，由测试负责人变更线上问题状态，从待测试-->无效；

4.5.待上线
线上问题经过测试负责人验证后：

已经修复，由测试负责人变更线上问题状态，从待测试→待上线；
未修复，发起线上问题打回流程，由测试负责人变更线上问题状态，从待测试-->开发中。
4.6.已关闭
线上问题经过测试负责人上线后，线上验证已经修复后，由测试负责人变更线上问题状态，从待上线-->已关闭；

5.线上问题填写
任何角色发现线上问题，都可以进行线上问题过程进度管理；

5.1.线上问题处理规范
线上问题不论大小，第一时间响应；
有线上问题先抛到线上问题&故障上报群，上升通知leader；
首先确定最近是否有上线，有上线的话第一时间回滚；
电话联系客户端RD、API RD及对应业务后端RD定位问题，无人响应上升电话联系leader；
确定是不是客户端问题，是不是新版本问题，如果是客户端问题，确定老包有无问题，若老包无问题，及时止损；
如果不是客户端问题，和RD并行进行问题定位：charles抓线上包拿到request id，拿request id查询全链路；
将全链路日志发到故障群定位；
线下缺陷管理是怎么，具体什么流程,"[""线下缺陷分级"", ""线下缺陷分类"", ""线下缺陷状态"", ""线下缺陷责任划分"", ""线下缺陷填写""]","1.线下缺陷分级
1.1.P0-致命线下缺陷
不能完全满足系统要求，系统停止运行，系统的重要部件无法运行，系统崩溃或者挂起等导致系统不能正常运行。修改优先级为最高，该级别问题需要立即修改。例如:

1.系统崩溃

2.导致程序重启,死机或非法退出

3.死循环

4.数据丢失或异常

5.数据通讯错误。

6.硬件故障，系统悬挂

1.2.P1-严重线下缺陷
严重地影响系统要求或基本功能的实现，且没有更正办法（重新安装或重新启动该软件不属于更正办法）。使系统不稳定、或破坏数据、或产生错误结果，或部分功能无法执行，而且是常规操作中经常发生或非常规操作中不可避免的主要问题，系统无法满足主要的业务要求，性能、功能或可用性严重降低。 修改优先级为高，该级别需要程序员尽快修改。 例如：

1.功能不符合用户需求

2.数据计算错误

3.业务流程错误

4.程序接口错误

5.因错误操作迫使程序中断；

6.系统可被执行，但操作功能无法执行（含指令）；

7.功能项的某些项目（选项）使用无效（对系统非致命的）；

8.功能实现不完整，如删除时没有考虑数据关联；

9.功能的实现不正确，如在系统实现的界面上，一些可接受输入的控件点击后无作用，对数据库的操作不能正确实现。

1.3.P2-一般线下缺陷
系统可以满足业务要求，系统性能或响应时间变慢、产生错误的中间结果但不影响最终结果等影响有限的问题。 修改优先级为中，该级别需要程序员修改。 例如：

1.数据长度不一致

2.内容或格式错误

3.响应时间较慢

4.功能性建议

5.提示信息不太准确

6.操作界面错误（包括数据窗口内列名定义、含义是否一致）；

7.简单的输入限制未放在前台进行控制；

8.虽然正确性不受影响，但系统性能和响应时间受到影响；

9.不能定位焦点或定位有误，影响功能实现；

10.增删改功能，在本界面不能实现，但在另一界面可以补充实现。

1.4.P3-低级线下缺陷
使操作者不方便或操作麻烦，但它不影响执行工作功能或重要功能。界面拼写错误或用户使用不方便等小问题或需要完善的问题。修改优先级为低，该级别需要程序员修改或不修改。

1.界面不规范；

2.辅助说明描述不清楚；

3.输入输出不规范；

4.长时间操作未给用户提示；

5.提示窗口文字未采用行业术语；

6.可输入区域和只读区域没有明显的区分标志；

7.必填项与非必填项应加以区别；

8.滚动条无效；

9.键盘支持不好，如在可输入多行的字段中，不支持回车换行；

10.界面不能及时刷新，影响功能实现。

1.5.P4-建议线下缺陷
希望提出的建议以及建议进行但不强制进行的修改。不会给发布的准确性或可用性带来任何严重影响。 修改优先级为低，该级别需要程序员修改或不修改。 例如：

1.各种提示框信息使用不统一,未采用行业术语

2.界面显示或描述建议

3.光标跳转设置不好，鼠标（光标）定位错误；

4.其他建议性问题。

2.线下缺陷分类
2.1.需求缺陷
需求文档中缺少相应描述，需求变更；

2.2.功能逻辑
功能上的错误性线下缺陷；

2.3.安全漏洞
XSS注入，SQL注入，跨域等问题；

2.4.性能问题
服务端性能：响应时间过长，CPU过高，GC频繁，没有分页，没有缓存等；

客户端性能：包大小，冷启动时间，流量，内存泄漏，加载时间，耗电量，页面加载等；

2.5.环境问题
被测服务环境不稳定，测试配置等；

2.6.数据问题
错误数据，不完整数据等；

2.7.交互UI
页面设计与交互UI不符等；

2.8.外部问题
依赖的外部系统引入的问题等；

2.9.技术缺陷
技术方案考虑不足，代码异常处理不足，前后端接口定义不一致，对边界、异常场景考虑不全等；

2.10.兼容性问题
3.线下缺陷状态
3.1.待开发
线下缺陷创建时初始状态，线下缺陷流转至研发负责人；

3.2.待测试
线下缺陷经过研发负责人处理解决后，研发负责人将线下缺陷状态从待开发置为待测试，线下缺陷流转至测试人负责人；

3.3.已关闭
线下缺陷经过测试负责人验证后：

线下缺陷已经修复，测试负责人将线下缺陷状态从待测试置为已关闭，流程结束；
线下缺陷未修复，测试负责人将线下缺陷状态从待测试置为待开发，即测试负责人将线下缺陷打回，线下缺陷流转至研发负责人。
3.4.无效
线下缺陷经过测试负责人验证后，发现线下缺陷是无效线下缺陷，测试负责人将线下缺陷状态从待测试置为无效，流程结束；

4.责任划分
4.1.责任角色
该线下缺陷由哪个角色导致的，比如产品经理，前端人员，IOS，安卓，服务端，测试人员等；

4.2.研发负责人
该线下缺陷由哪些人负责修复，该人的角色可以是产品经理，前端人员，IOS，安卓，服务端，测试人员等；

4.3.测试负责人
该线下缺陷有哪些人负责验证，该人的角色可以是测试人员，产品经理，前端人员，IOS，安卓，服务端等；

5.线下缺陷填写
任何角色发现线下线下缺陷，都通过ones平台进行记录，测试进行线下缺陷的过程进度管理"
发布规范有哪些,,"1，代码分支：代码分支相关，线上发布的分支，必须是master分支
2，发布物：线上发布的包，必须是经过线下验证过的，尤其是后端，涉及SQL发布的，必须验证SQL执行的结果和预期一致
3，发布过程：发布时，研发QA要在现场，根据checklist顺次上线，正常观察半个小时日志正常，有跑批的要观察第一次跑批正常。不允许发布完后不再观察。发布过程中，要在群里同步开始以及发布完成。
4，发布窗口：周二周四。非发布窗口，必须给对应的研发leader，QA leader以及中心负责人报备和评估。特殊业务可以单独开窗口，需中心负责人同意。封网期间，原则上不允许发版，如果有特殊情况，也是需要向中心负责人报备同意
5，发布值守：以应对突发状况，一个业务需求，同时发布P0/P1应用大于3个，就需要值守，值守时间第二天早上9点开始，值守方式（在公司or在家）
6，发布回滚：止损第一位，线上出现问题，立马根据checklist评审的内容，在5分钟内回滚完成。回滚不用经过leader审批，owner直接可以操作。回滚后，master代码分支要在一定时间内回退
7，hotfix发布：务必合并到master分支进行发布。如果是在窗口期，不需要报备。非窗口期也需要按照非窗口期的流程报备，报备主体是研发
8，紧急需求发布：如果在非窗口期，需要正常报备，并打开审批流程，报备主体是产品
9，发布checklist：涉及p0级别服务发布的，必须要有checklist，checklist中内容也要有要求：变更应用清单，变更配置清单、发布顺序、在途单子是否影响，告警配置，容量评估（资源观察），回滚步骤（包括应用和数据）
10，发布验证：产品除了在线下验收，也要在生产验证，研发QA可以协助，同时研发测试和产品沟通，关注投产情况。同时开城也属于发布验证的延续，运营开城后，需要通知产研侧观察线上运行情况
11，新服务发布：资源申请原则上一倍冗余，以及相关权限的开通，如日志，告警等，操作权限的研发>=2人
12，每次发布，要有一个release owner，改动大的一方作为release owner，从上线到投产开城，一直要关注，整个需求的生命周期
其他：
1，多栈代码，由服务owner操作，上线，代码合并和回退，回滚
2，增长服务，在增长类目内，原则上没有窗口限制
3，无论是技术需求还是产品需求，必须要有ones，尽量不要复用ones
4，大家下班要带着电脑，另外就是要保证vpn的正常登录"
checklist规范有哪些,,"1、研发owner牵头，整体把控上线checklist编写，为checklist的完备性及执行负责
2、checklist最晚[ddl]需在上线日的T-1准备好；<为保证checklist质量，可在更前置的环节如技术评审时、showcase后整理好checklist并至少进行doublecheck>
3、评估checklist是否需要评审，对checklist内容进行review及补充，最晚需在上线日T-1评审完成并完成内容补充
4、测试上线前确认checklist是否完善，并做上线前把控，无checklist不允许上线"
为什么要有checklist,,checklist旨在督促大家上线前做变更确认
技术需求需要checklist吗,,需要
变更点很少checklist不需要维护,,错误
checklist越细化越好,,是的
如何进行codereview，怎么操作？,"[""CR的流程是什么？"", ""codereview流程是什么？""]","1) 开发者完成开发，提测之前，通过gitlab建立merge request，发起从个人分支到develop-xx测试分支合并，企业微信通知CR审核人（禁止私自直接点击merge，CR执行同学互相merge，发现私自merge，惩罚措施组内协商制度）如果发现问题，需要在问题处做标注，所有标注完成后，通知CR提交人进行修改问题修复。开发者修复后，CR人继续审核，直到通过并点击合并为止；

2) CR审查人进行代码审核，并点击合并按钮；

3) QA测试完毕，上线前。开发者通过gitlab建立merge request，发起从develop分支到master合并，企业微信通知CR审核人；

3) CR审查人进行代码审核，审查完毕如没有问题则在页面底部写上固定注释（确认无误可以上线），并点击合并按钮。"
CR通过标准是什么？,"[""怎么算cr通过"", ""codereview怎么算是通过了"", ""coderiview通过"", ""cr通过"", ""CR通过""]","不再以下两种情况的都可以理解为通过
1、字面量常量化定义，变量名、函数命名不规范，注释不清楚或缺少注释，代码风格不正确；
2、代码块修改，函数重构、逻辑补充、异常处理等；"
showcase通过的标准是什么？,"[""showcase怎么算通过"", "" showcase怎么算是通过了"", ""showcase通过"", ""showcase通过参考依据"", "" showcase通过依据""]",QA或PM记录SHOWCASE期间的问题，标记CASE中90%通过则算通过
showcase不通过后续怎么处理？,"[""showcase不过怎么办"", "" showcase不通过""]","若SHOWCASE不通过：研发需给出第二次SHOWCASE时间，若第二次SHOWCASE通过，则进入测试阶段。若仍未通过，则进行第三轮SHOWCASE。
若在约定时间内未SHOWCASE，则视为提测延期，若影响后续进度，可重新排期"
showcase的用例是怎么定的？,"[""showcase的范围是什么"", "" showcase用例来源""]","SHOWCASE范围：
业务类：
与研发自测范围一致（CASE评审三方达成一致的标注CASE）
建议主流程作为基准去覆盖（明确给出具体case的占比情况各业务方向自己确定）
数据类：

数据类showcase，按照数据类型或数据场景覆盖主流程（不用按照业务类需求完全一致）
环境：需在测试环境执行SHOWCASE

数据：SHOWCASE均为真实链路中的测试数据，不可mock数据「特殊情况需产研测达成一致」"
showcase谁来组织的？,"[""showcase发起角色是什么"", "" showcase谁发起""]",研发
分支管理流程是什么？,"[""分支提交标准是什么"", "" 分支怎么管理"", ""分支提交相关要求是什么"", ""分支管理""]","开发新feature(包含每周常规)：
1、开发人员基于master分支创建feature分支，在feature分支进行开发；
2、开发人员完成新feature开发后，开发人员基于master分支创建test分支，并将feature分支代码合入test分支，然后提测；
备注：若同一项目的一次迭代包含多个新feature，不同feature开发完成后，统一合入同一个test分支；
3、测试人员基于test分支进行新功能测试，并在每一轮测试开始前，测试人员把当前master分支代码合入test分支，然后再进行测试；
备注：测试过程中发现的bug，开发人员修复代码统一提到test分支
4、测试人员基于test分支，测试完成后，研发人员把test分支merge到master分支，并对master分支打tag，然后用该tag上线；"
前端自测一般关注哪些点？,"[""前端怎么自测"", "" 前端自测标准是什么"", ""前端自测关注什么"", ""前端自测注意什么"", ""前端自测重点关注什么""]","功能测试

文案、语义检查；
导航、菜单、链接检查；
接口访问异常提示检查；
性能自测

接口返回速度；
JS执行速度；
兼容性自测

chrome浏览器；
界面、ajax；
容错自测

输入框最长输入；
表单重复提交；
表单错误类型的内容提交；
特殊字符输入；
安全自测

常见的xss攻击，例如 <img src=1 onerror='alert(1)'>；
交互自测

输入框的自动获取焦点；
搜索按钮的回车事件；
注：前端开发依赖mock接口，可以边开发边进行自测。"
后端自测要求？,"[""后端自测方式"", "" 后端怎么自测"", ""服务端自测"", ""后端自测标准""]","后端服务代码要求接口层、业务逻辑层的单元测试覆盖率至少在70%以上；
1）使用单元测试工具和框架，进行接口级别，类级别，函数级别，行级别的单元测试《研发域_规范_单元测试》。
2）使用postman或者其他类似工具，进行接口模拟测试，保证提供接口正常"
为什么要提前联调,"[""提前联调目的"", ""联调目的"", ""为什么联调""]",重要目的是发现问题、解决问题。解决问题的前提是定位问题，而定位问题的区域范围越小，就越容易用较小的成本解决问题；
研发联调需要做什么准备,"[""开发联调前要准备哪些技术清单"", ""研发联调"", ""研发联调注意事项""]","接口文档准备完善
联调case准备完成；
联调环境准备完成；
联调数据准备完成
实数据导入(协调数据源的同步)

假数据导入(假如不能导入真实数据，需要造数据)"
单测结构规范是什么,"[""单元测试有什么规范要求"", ""单测模块建立规范是什么"", ""单测目录建议是什么样""]","1.1.目录结构规范
1.源码存放在src目录，每个功能模块创建单个包
2.src同级创建test/unit作为单元测试文件目录
3.test/unit目录下创建源包同名文件夹，用于存放单元测试文件
4.src同级创建test/integration作为集成测试文件夹
5.test/integration目录下创建源包同名文件夹，用于存放集成测试文件
1.2.文件命名规范
1.test目录下测试文件名同源码文件名同名，后缀以.test.js结尾
2.test/unit和test/integration创建测试文件夹时，参照短横线（kabab-case）规范命名。
3.js和ts文件依照短横线（kabab-case）规范命名，Vue文件依照驼峰（camelCased）规范命名。
4.每个源码文件（如js，ts，vue）对应一个同名.test.js文件。（index文件可以忽略）"
单测编码规范是什么,"[""单元测试编码规范"", ""单测编码"", ""单元测试编码方案"", ""单元测试""]","1.test对应每个源码创建单元测试包；
2.每个包，都要添加descript,描述名为包名；
3.需生成快照文件的单元测试，快照需要每次提交；
4.expect test('') 中描述的是调用和期望输出结果；
5.进行参数或属性校验；
校验包含正向和反向校验，即正确类型正确输出和异常类型返回异常信息等。
校验种类包含，参数个数、参数类型等。

6.测试要覆盖实现中的代码的各个分支；
7.一个测试方法只测试一个方法，不测试私有方法；
8.一个测试类只对应一个被测类；
9.测试用例的变量和方法都要有明确的含义；"
单元测试覆盖率参考值是什么,"[""单元测试覆盖率有要求吗"", ""单元测试覆盖率怎么定"", ""单测覆盖率要求"", ""单测覆盖率""]","说明：以下仅作为参考，实际还需要按照各自项目进行评估。
语句覆盖率：>=70%
分支覆盖率：100%
函数覆盖率：100%
行覆盖率： >=80%
执行覆盖率标准在 80% 左右。"
单元测试重点关注哪些场景,"[""单元测试关注场景"", ""单测场景"", ""单元测试""]","1.1.接口功能性测试
接口功能的正确性,即保证接口能够被正常调用，并输出有效数据：
是否被顺利调用
参数是否符合预期
1.2.局部数据结构测试
保证数据结构的正确性：
变量是否有初始值或在某场景下是否有默认值
 变量是否溢出
1.3.边界条件测试
变量无赋值(null)
变量是数值或字符
主要边界：最大值，最小值，无穷大
溢出边界：在边界外面取值+/-1
临近边界：在边界值之内取值+/-1
字符串的边界，引用 ""变量字符""的边界
字符串的设置，空字符串
字符串的应用长度测试
空白集合
目标集合的类型和应用边界
集合的次序
变量是规律的，测试无穷大的极限，无穷小的极限"
金融支付军规是什么？,"[""支付有什么红黑线"", ""金融红黑线是什么"", ""支付金融军规是什么"", ""支付军规"", ""金融支付有军规吗，是什么？""]","1、关键系统代码CR必审核
2、L1系统告警1分钟响应
3、关键系统上线值守
4、SQL必审核，执行必闭环
5、封网期线上禁变更，变更必审
6、变更范围与需求一致
7、上线计划有checklist
8、线上问题优先止损
9、非审批授权敏感数据禁止导出
10、线上XXLJOB禁止手动注册
11、线上接口调用必走RDBOSS"
多栈统计标准是什么,"[""什么需求算是多栈"", ""多栈需求怎么统计？"", ""多栈口径是什么"", ""多栈标准是什么""]","1、跨原始角色排期
2、有大于0的排期"
资金类需求定义是什么,"[""怎么算是资金类需求"", ""资金类需求是什么"", ""资金需求定义"", ""资金定义是什么""]",影响到资金收支，放款回款以及账务资金，或者业务上有资金计算逻辑，判断逻辑等的需求，即可定义为资金类需求
资金类需求设计原理,"[""资金类需求设计"", ""资金类需求怎么设计"", ""资金安全""]","1，需求评审时，需要加上是否为资金类需求或是否涉及资金类需求

2，需求中涉及资金部分的内容要突出标识

3，资金类需求的产品方案，要涵盖资金正常，异常，手工等业务逻辑流程

4，资金类需求的产品方案，须特别考虑上下游系统的数据一致性，并强制增加校验逻辑

5，资金类需求的产品方案，做闭环设计，包含：对业务流，资金流，凭证的核对逻辑

6，涉及到资金出金类的，必须明确成功、失败、中间态的所有的状态码

7，涉及资金出金相关的系统功能需要考虑用户权限限制

8，涉及资金出金相关的接口交互，接口字段需要明确到字段级，尤其重要时间点、状态等

9，涉及资金出金相关的需求，除了改造部分，还要考虑历史版本逻辑

10，涉及资金出金相关的需求，除产品方案，还应有试点及切换方案"
出金类交易的隐患点是什么,"[""出金类交易常见隐患点是什么"", ""出金类隐患是什么"", ""出金系统隐患是什么"", ""出金类交易隐患点""]","A.幂等性问题
B.事务处理错误
C.交易状态映射不清晰
D.并发控制问题"
为了避免重复记账，系统常采用哪些措施,"[""怎么避免重复记账"", ""重复记账怎么设计能避免"", ""重复记账规避""]","A.使用分布式锁
B.使用唯一索引、确保每笔交易只能记录一次
C.定期审计、检查是否存在重复记录
D.增加重复记账异常告警"
